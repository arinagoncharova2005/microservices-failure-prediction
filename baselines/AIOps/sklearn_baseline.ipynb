{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIOps: sklearn models baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "def read_csv_dataset(file_path, sep=','):\n",
        "    return pd.read_csv(file_path, sep=sep)\n",
        "\n",
        "def convert_timestamps_to_datetime(df, timestamp_column_name='timestamp', timestamp_unit='s'):\n",
        "    df['datetime'] = pd.to_datetime(df[timestamp_column_name], unit=timestamp_unit)\n",
        "    return df\n",
        "\n",
        "def get_data(file_path, sep=',', timestamp_column_name='timestamp', timestamp_unit='s'):\n",
        "    df = read_csv_dataset(file_path, sep=sep)\n",
        "    if timestamp_column_name in df.columns:\n",
        "        df = convert_timestamps_to_datetime(df, timestamp_column_name, timestamp_unit)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def normalize_log(line: str) -> str:\n",
        "    return re.sub(r\"\\d+\", \"<NUM>\", str(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main functions for clustering and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (classification_report, roc_auc_score, average_precision_score,\n",
        "                             precision_recall_curve, f1_score)\n",
        "\n",
        "import hdbscan\n",
        "from hdbscan import approximate_predict\n",
        "\n",
        "@dataclass\n",
        "class WindowConfig:\n",
        "    # cluster agg frequency\n",
        "    freq: str = \"1min\"     \n",
        "    # history length \n",
        "    history_len: int = 5 \n",
        "    pre_failure_lead: pd.Timedelta = pd.Timedelta(\"30min\")\n",
        "    post_incident_ignore: pd.Timedelta = pd.Timedelta(\"0min\")\n",
        "\n",
        "@dataclass\n",
        "class ClusterConfig:\n",
        "    metric: str = \"euclidean\"\n",
        "    min_cluster_size: int = 800\n",
        "    min_samples: int = 400\n",
        "    cluster_selection_method: str = \"eom\"\n",
        "    cluster_selection_epsilon: float = 0.03\n",
        "    prediction_data: bool = True\n",
        "    fit_max_points: Optional[int] = None\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    random_state: int = 42\n",
        "    class_weight: str = \"balanced\"\n",
        "    max_iter: int = 1000\n",
        "\n",
        "def make_labels_by_time(times: pd.Series, incidents: pd.DataFrame, lead: pd.Timedelta,\n",
        "                        service: Optional[pd.Series] = None) -> pd.Series:\n",
        "    times = pd.to_datetime(times)\n",
        "    y = pd.Series(0, index=times.index, dtype=int)\n",
        "    if service is None or \"service\" not in incidents.columns:\n",
        "        inc_times = pd.to_datetime(incidents[\"datetime\"]).sort_values().values\n",
        "        for t0 in inc_times:\n",
        "            mask = (times > (t0 - lead)) & (times <= t0)\n",
        "            y.loc[mask] = 1\n",
        "    else:\n",
        "        for srv, grp in incidents.groupby(\"service\"):\n",
        "            inc_times = pd.to_datetime(grp[\"datetime\"]).sort_values().values\n",
        "            srv_mask = (service == srv).values\n",
        "            t_srv = times[srv_mask]\n",
        "            idx_srv = times.index[srv_mask]\n",
        "            y_srv = pd.Series(0, index=idx_srv, dtype=int)\n",
        "            for t0 in inc_times:\n",
        "                mask = (t_srv > (t0 - lead)) & (t_srv <= t0)\n",
        "                y_srv.loc[mask] = 1\n",
        "            y.loc[idx_srv] = y_srv.values\n",
        "    return y\n",
        "\n",
        "def fit_hdbscan_on_train(embeddings: np.ndarray, cfg: ClusterConfig) -> hdbscan.HDBSCAN:\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=cfg.metric,\n",
        "        min_cluster_size=cfg.min_cluster_size,\n",
        "        min_samples=cfg.min_samples,\n",
        "        cluster_selection_method=cfg.cluster_selection_method,\n",
        "        cluster_selection_epsilon=cfg.cluster_selection_epsilon,\n",
        "        prediction_data=cfg.prediction_data,\n",
        "    )\n",
        "    if cfg.fit_max_points is not None and embeddings.shape[0] > cfg.fit_max_points:\n",
        "        idx = np.random.RandomState(0).choice(embeddings.shape[0], cfg.fit_max_points, replace=False)\n",
        "        clusterer.fit(embeddings[idx])\n",
        "    else:\n",
        "        clusterer.fit(embeddings)\n",
        "    return clusterer\n",
        "\n",
        "def predict_clusters(clusterer: hdbscan.HDBSCAN, embeddings: np.ndarray):\n",
        "    labels, strengths = approximate_predict(clusterer, embeddings)\n",
        "    return labels, strengths\n",
        "\n",
        "def resample_and_pivot_counts(df: pd.DataFrame, time_col: str = \"datetime\", cluster_col: str = \"cluster\",\n",
        "                              freq: str = \"1min\", service_col: Optional[str] = None) -> pd.DataFrame:\n",
        "    df2 = df.copy()\n",
        "    df2[time_col] = pd.to_datetime(df2[time_col])\n",
        "    group_keys = [pd.Grouper(key=time_col, freq=freq), cluster_col]\n",
        "    index_cols = [time_col]\n",
        "    if service_col and service_col in df2.columns:\n",
        "        group_keys = [service_col] + group_keys\n",
        "        index_cols = [service_col, time_col]\n",
        "    per_bucket = df2.groupby(group_keys).size().reset_index(name=\"count\")\n",
        "    pivot = per_bucket.pivot_table(index=index_cols, columns=cluster_col, values=\"count\", fill_value=0, aggfunc=\"sum\").reset_index()\n",
        "    pivot.columns = [str(c) for c in pivot.columns]\n",
        "    return pivot\n",
        "\n",
        "def add_history_features(pivot: pd.DataFrame, time_col: str = \"datetime\", service_col: Optional[str] = None,\n",
        "                         history_len: int = 5) -> pd.DataFrame:\n",
        "    df = pivot.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    cluster_cols = [c for c in df.columns if c not in ([time_col] + ([service_col] if service_col else []))]\n",
        "    def _add_roll(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        g = g.sort_values(time_col)\n",
        "        for c in cluster_cols:\n",
        "            g[f\"{c}_mean{history_len}\"] = g[c].rolling(window=history_len, min_periods=1).mean()\n",
        "            g[f\"{c}_diff1\"] = g[c].diff().fillna(0)\n",
        "        return g\n",
        "    if service_col and service_col in df.columns:\n",
        "        df = df.groupby(service_col, group_keys=False).apply(_add_roll)\n",
        "    else:\n",
        "        df = _add_roll(df)\n",
        "    return df\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    train_start: pd.Timestamp\n",
        "    train_end: pd.Timestamp\n",
        "    test_start: pd.Timestamp\n",
        "    test_end: pd.Timestamp\n",
        "\n",
        "def build_time_splits(times: pd.Series, n_splits: int = 3, min_train_period: str = \"14D\", test_period: str = \"7D\") -> List[Split]:\n",
        "    times = pd.to_datetime(times).sort_values()\n",
        "    start, end = times.min(), times.max()\n",
        "    splits: List[Split] = []\n",
        "    train_end = start + pd.Timedelta(min_train_period)\n",
        "    for _ in range(n_splits):\n",
        "        test_start = train_end\n",
        "        test_end = min(test_start + pd.Timedelta(test_period), end)\n",
        "        if test_start >= test_end:\n",
        "            break\n",
        "        splits.append(Split(start, train_end, test_start, test_end))\n",
        "        train_end = test_end  # expanding\n",
        "    return splits\n",
        "\n",
        "def pick_threshold_by_f1(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
        "    if len(thr) == 0:\n",
        "        return 0.5\n",
        "    f1s = (2 * prec[:-1] * rec[:-1]) / np.clip(prec[:-1] + rec[:-1], 1e-9, None)\n",
        "    best = int(np.nanargmax(f1s))\n",
        "    return float(thr[best])\n",
        "\n",
        "@dataclass\n",
        "class EventMetrics:\n",
        "    detection_rate: float\n",
        "    mean_lead_minutes: float\n",
        "    false_alarms_per_day: float\n",
        "    threshold: float\n",
        "\n",
        "def evaluate_events(df_scores: pd.DataFrame, incidents: pd.DataFrame, window_cfg: WindowConfig,\n",
        "                    service_col: Optional[str] = None, time_col: str = \"datetime\", proba_col: str = \"proba\",\n",
        "                    threshold: float = 0.5) -> EventMetrics:\n",
        "    d = df_scores.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col])\n",
        "    d = d.sort_values(time_col)\n",
        "    detections = 0\n",
        "    leads: List[float] = []\n",
        "    total_days = (d[time_col].max() - d[time_col].min()).total_seconds() / 86400.0\n",
        "    alerts = d[d[proba_col] >= threshold].copy()\n",
        "    is_true_alert = pd.Series(False, index=alerts.index)\n",
        "    if service_col and service_col in d.columns and \"service\" in incidents.columns:\n",
        "        for srv, inc_grp in incidents.groupby(\"service\"):\n",
        "            d_srv = d[d[service_col] == srv]\n",
        "            alerts_srv = alerts[alerts[service_col] == srv]\n",
        "            for _, row in inc_grp.iterrows():\n",
        "                t0 = pd.to_datetime(row[\"datetime\"])\n",
        "                win_mask = (d_srv[time_col] > (t0 - window_cfg.pre_failure_lead)) & (d_srv[time_col] <= t0)\n",
        "                win_alerts = d_srv[win_mask & (d_srv[proba_col] >= threshold)]\n",
        "                if not win_alerts.empty:\n",
        "                    detections += 1\n",
        "                    first_alert = win_alerts[time_col].min()\n",
        "                    leads.append((t0 - first_alert).total_seconds() / 60.0)\n",
        "                idx_true = d_srv.index[win_mask & (d_srv[proba_col] >= threshold)]\n",
        "                is_true_alert.loc[alerts_srv.index.intersection(idx_true)] = True\n",
        "    else:\n",
        "        for _, row in incidents.iterrows():\n",
        "            t0 = pd.to_datetime(row[\"datetime\"])\n",
        "            win_mask = (d[time_col] > (t0 - window_cfg.pre_failure_lead)) & (d[time_col] <= t0)\n",
        "            win_alerts = d[win_mask & (d[proba_col] >= threshold)]\n",
        "            if not win_alerts.empty:\n",
        "                detections += 1\n",
        "                first_alert = win_alerts[time_col].min()\n",
        "                leads.append((t0 - first_alert).total_seconds() / 60.0)\n",
        "            idx_true = d.index[win_mask & (d[proba_col] >= threshold)]\n",
        "            is_true_alert.loc[alerts.index.intersection(idx_true)] = True\n",
        "    false_alarms = int((~is_true_alert).sum())\n",
        "    return EventMetrics(\n",
        "        detection_rate = detections / max(len(incidents), 1),\n",
        "        mean_lead_minutes = float(np.mean(leads)) if leads else 0.0,\n",
        "        false_alarms_per_day = false_alarms / max(total_days, 1e-9),\n",
        "        threshold = threshold,\n",
        "    )\n",
        "\n",
        "def train_eval_timewindow(df_logs: pd.DataFrame, incidents: pd.DataFrame,\n",
        "                          window_cfg: WindowConfig = WindowConfig(),\n",
        "                          cluster_cfg: ClusterConfig = ClusterConfig(),\n",
        "                          model_cfg: ModelConfig = ModelConfig(),\n",
        "                          service_col: Optional[str] = None,\n",
        "                          time_col: str = \"datetime\",\n",
        "                          embedding_col: str = \"embedding\",\n",
        "                          n_splits: int = 3, min_train_period: str = \"14D\", test_period: str = \"7D\") -> Dict[str, Any]:\n",
        "    df = df_logs.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df = df.sort_values(time_col)\n",
        "    splits = build_time_splits(df[time_col], n_splits=n_splits, min_train_period=min_train_period, test_period=test_period)\n",
        "    if not splits:\n",
        "        raise ValueError(\"Not enough data for the requested splits.\")\n",
        "    results = []\n",
        "    chosen_thr: Optional[float] = None\n",
        "    for k, sp in enumerate(splits):\n",
        "        train_mask = (df[time_col] >= sp.train_start) & (df[time_col] < sp.train_end)\n",
        "        test_mask  = (df[time_col] >= sp.test_start) & (df[time_col] < sp.test_end)\n",
        "        train_df = df.loc[train_mask].copy()\n",
        "        test_df  = df.loc[test_mask].copy()\n",
        "        X_train_emb = np.vstack(train_df[embedding_col].to_list())\n",
        "        print('hdbscan fitting fold', k+1)\n",
        "        clusterer = fit_hdbscan_on_train(X_train_emb, cluster_cfg)\n",
        "        print('hdbscan predicting fold', k+1)\n",
        "        train_labels, train_strengths = predict_clusters(clusterer, X_train_emb)\n",
        "        train_df[\"cluster\"] = train_labels\n",
        "        train_df[\"cluster_strength\"] = train_strengths\n",
        "        X_test_emb = np.vstack(test_df[embedding_col].to_list())\n",
        "        test_labels, test_strengths = predict_clusters(clusterer, X_test_emb)\n",
        "        test_df[\"cluster\"] = test_labels\n",
        "        test_df[\"cluster_strength\"] = test_strengths\n",
        "        pivot_train = resample_and_pivot_counts(train_df, time_col=time_col, cluster_col=\"cluster\", freq=window_cfg.freq, service_col=service_col)\n",
        "        pivot_test  = resample_and_pivot_counts(test_df,  time_col=time_col, cluster_col=\"cluster\", freq=window_cfg.freq, service_col=service_col)\n",
        "        feat_train = add_history_features(pivot_train, time_col=time_col, service_col=service_col, history_len=window_cfg.history_len)\n",
        "        feat_test  = add_history_features(pivot_test,  time_col=time_col, service_col=service_col, history_len=window_cfg.history_len)\n",
        "        y_train = make_labels_by_time(feat_train[time_col], incidents, lead=window_cfg.pre_failure_lead,\n",
        "                                      service=feat_train[service_col] if service_col and service_col in feat_train.columns else None)\n",
        "        y_test  = make_labels_by_time(feat_test[time_col],  incidents, lead=window_cfg.pre_failure_lead,\n",
        "                                      service=feat_test[service_col]  if service_col and service_col in feat_test.columns else None)\n",
        "        drop_cols = [time_col] + ([service_col] if service_col and service_col in feat_train.columns else [])\n",
        "        print('training classifier fold', k+1)\n",
        "        X_train = feat_train.drop(columns=drop_cols)\n",
        "        X_test  = feat_test.drop(columns=drop_cols)\n",
        "        pipe = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=False)),\n",
        "            (\"clf\", LogisticRegression(max_iter=model_cfg.max_iter, class_weight=model_cfg.class_weight, random_state=model_cfg.random_state))\n",
        "        ])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        proba_train = pipe.predict_proba(X_train)[:, 1]\n",
        "        proba_test  = pipe.predict_proba(X_test)[:, 1]\n",
        "        if chosen_thr is None:\n",
        "            chosen_thr = pick_threshold_by_f1(y_train.values, proba_train)\n",
        "        pred_test = (proba_test >= chosen_thr).astype(int)\n",
        "        roc = roc_auc_score(y_test, proba_test) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        pr_auc = average_precision_score(y_test, proba_test) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        f1 = f1_score(y_test, pred_test) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        rep = classification_report(y_test, pred_test, digits=3, zero_division=0)\n",
        "        df_scores = feat_test[[time_col]].copy()\n",
        "        df_scores[\"proba\"] = proba_test\n",
        "        if service_col and service_col in feat_test.columns:\n",
        "            df_scores[service_col] = feat_test[service_col].values\n",
        "        em = evaluate_events(df_scores, incidents, window_cfg, service_col=service_col, time_col=time_col, proba_col=\"proba\", threshold=chosen_thr)\n",
        "        results.append({\n",
        "            \"fold\": k+1,\n",
        "            \"train_range\": (sp.train_start, sp.train_end),\n",
        "            \"test_range\": (sp.test_start, sp.test_end),\n",
        "            \"roc_auc\": roc,\n",
        "            \"pr_auc\": pr_auc,\n",
        "            \"f1_at_thr\": f1,\n",
        "            \"thr\": chosen_thr,\n",
        "            \"event_detection_rate\": em.detection_rate,\n",
        "            \"mean_lead_minutes\": em.mean_lead_minutes,\n",
        "            \"false_alarms_per_day\": em.false_alarms_per_day,\n",
        "            \"classification_report\": rep,\n",
        "        })\n",
        "    return {\"results\": results, \"chosen_threshold\": chosen_thr, \"window_cfg\": window_cfg, \"cluster_cfg\": cluster_cfg}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (50, 5)\n",
            "             datetime                cmdb_id\n",
            "0 2022-05-02 19:31:46  productcatalogservice\n",
            "1 2022-05-02 20:32:34  recommendationservice\n",
            "2 2022-05-02 22:12:54               frontend\n",
            "3 2022-05-02 23:14:17  recommendationservice\n",
            "4 2022-05-02 23:52:18            cartservice\n",
            "Dataset shape: (5444332, 6)\n",
            "                 log_id   timestamp        cmdb_id  \\\n",
            "0  Cp6Bt38B8vQa58bZsQau  1651507200     frontend-1   \n",
            "1  EZ6Bt38B8vQa58bZqQWr  1651507200  cartservice-2   \n",
            "2  FZ6Bt38B8vQa58bZqQWr  1651507200  cartservice-2   \n",
            "3  Fp6Bt38B8vQa58bZqQWr  1651507200  cartservice-2   \n",
            "4  F56Bt38B8vQa58bZqQWr  1651507200  cartservice-2   \n",
            "\n",
            "                              log_name  \\\n",
            "0     log_frontend-service_application   \n",
            "1  log_cartservice-service_application   \n",
            "2  log_cartservice-service_application   \n",
            "3  log_cartservice-service_application   \n",
            "4  log_cartservice-service_application   \n",
            "\n",
            "                                               value            datetime  \n",
            "0         severity: debug, message: request complete 2022-05-02 16:00:00  \n",
            "1       Executing endpoint 'gRPC - /hipstershop.C... 2022-05-02 16:00:00  \n",
            "2  [40m\u001b[32minfo\u001b[39m\u001b[22m\u001b[49m: Microsoft.AspNet... 2022-05-02 16:00:00  \n",
            "3       Request finished in 0.6231ms 200 applicat... 2022-05-02 16:00:00  \n",
            "4  [40m\u001b[32minfo\u001b[39m\u001b[22m\u001b[49m: Microsoft.AspNet... 2022-05-02 16:00:00  \n"
          ]
        }
      ],
      "source": [
        "# path to files\n",
        "INCIDENTS_CSV = '/Users/arinagoncharova/Documents/diploma/EDA/Aiops-Dataset/groundtruth/groundtruth-2022-05-03.csv'\n",
        "LOGS_CSV = '/Users/arinagoncharova/Documents/diploma/EDA/Aiops-Dataset/data/2022-05-03/log/all/log_filebeat-testbed-log-service.csv'\n",
        "\n",
        "failures_03_05_df = get_data(INCIDENTS_CSV)\n",
        "service_failures_03_05_df = failures_03_05_df[failures_03_05_df['level'] == 'service'].copy()\n",
        "\n",
        "# normalize service column name (use 'cmdb_id' if exists)\n",
        "service_col_name = None\n",
        "if 'cmdb_id' in service_failures_03_05_df.columns:\n",
        "    service_col_name = 'cmdb_id'\n",
        "else:\n",
        "    for cand in ['smbd_id', 'сmbd_id']:\n",
        "        if cand in service_failures_03_05_df.columns:\n",
        "            service_failures_03_05_df = service_failures_03_05_df.rename(columns={cand: 'cmdb_id'})\n",
        "            service_col_name = 'cmdb_id'\n",
        "            break\n",
        "\n",
        "incidents = service_failures_03_05_df.copy()\n",
        "need_cols = ['datetime'] + ([service_col_name] if service_col_name else [])\n",
        "incidents = incidents[need_cols].dropna().reset_index(drop=True)\n",
        "print(incidents.head())\n",
        "\n",
        "logs_03_05_service_df = get_data(LOGS_CSV)\n",
        "logs_df = logs_03_05_service_df.copy()\n",
        "if service_col_name and service_col_name not in logs_df.columns:\n",
        "    for cand in ['smbd_id', 'сmbd_id']:\n",
        "        if cand in logs_df.columns:\n",
        "            logs_df = logs_df.rename(columns={cand: 'cmdb_id'})\n",
        "            break\n",
        "assert 'value' in logs_df.columns, f\"Ожидается колонка 'value' в логах. Нашли: {list(logs_df.columns)}\"\n",
        "assert 'datetime' in logs_df.columns, \"Логи должны содержать временную метку (см. get_data).\"\n",
        "print(logs_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logs embeddings calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a87a942d868499e89108b94ab3174a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/2640 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "datetime",
                  "rawType": "datetime64[ns]",
                  "type": "datetime"
                },
                {
                  "name": "embedding",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "cmdb_id",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "ada30960-9166-486e-8f72-788800a71493",
              "rows": [
                [
                  "0",
                  "2022-05-02 16:00:00",
                  "[-6.92078322e-02  7.72757456e-02  1.93087272e-02  9.17874742e-03\n -1.59955788e-02  1.50614725e-02  5.06414771e-02 -5.00670560e-02\n -1.23837953e-02  6.03305846e-02  2.84830369e-02 -4.86314073e-02\n -1.46175679e-02  2.09096000e-02 -5.36488649e-03 -3.21712233e-02\n -2.32428238e-02  5.15080579e-02 -7.04149976e-02  4.51727323e-02\n  6.11654110e-02 -2.96720415e-02  2.03034579e-05 -2.44120955e-02\n  5.72165707e-03  9.94610693e-03  3.57718952e-02  9.09054875e-02\n -4.09669727e-02 -1.29950777e-01 -3.00217122e-02  2.90172659e-02\n  4.45260964e-02 -4.22430895e-02  8.62158388e-02 -4.46180291e-02\n -2.03058794e-02  3.18621285e-02  6.24350719e-02  8.49370286e-02\n -5.84058017e-02 -3.29208598e-02  1.53827723e-02 -6.74823672e-02\n  8.93405406e-04 -6.54926989e-03 -4.32875790e-02 -5.37765101e-02\n  6.78026900e-02 -4.77406429e-03  1.13288462e-02 -7.91468471e-03\n  3.52602825e-02  6.33150786e-02  3.52880247e-02  5.49490750e-02\n  5.77009842e-02  2.17326880e-02  3.70308794e-02  9.39905941e-02\n -6.27522124e-04  1.39896115e-02 -1.43928900e-01  1.49924532e-01\n  1.12008378e-01  3.37277539e-02 -5.88446632e-02  1.60358194e-02\n -4.12118509e-02  6.54914752e-02 -1.12902690e-02  2.98567321e-02\n -3.86079811e-02  8.14584866e-02 -5.82053214e-02  8.31409097e-02\n  7.64975930e-03  1.18319206e-02 -5.34892008e-02 -2.95928791e-02\n -1.27906667e-03 -7.32998997e-02 -1.93717405e-02  3.67709412e-03\n  8.08351114e-03 -5.43800592e-02  2.15933495e-03 -3.93813960e-02\n -3.13091464e-02 -1.06357019e-02 -5.67416400e-02 -4.19742353e-02\n  7.43137207e-03 -2.13969946e-02 -1.34867681e-02 -1.86315225e-03\n  3.09267566e-02 -6.43592095e-03  9.59985144e-03  8.90718177e-02\n  1.24789299e-02 -3.32545638e-02 -8.33169743e-03 -7.19366670e-02\n -1.43784899e-02 -3.66473831e-02  6.07745722e-02 -4.20257859e-02\n  5.21824695e-02  1.71186514e-02 -3.62918228e-02 -5.98217584e-02\n -1.99473239e-02 -3.15103233e-02  6.31147549e-02  2.71005668e-02\n  1.67324804e-02 -5.11871055e-02 -5.13516553e-02 -1.90145448e-02\n  8.15253183e-02  6.88283332e-03  2.61641257e-02 -3.06329709e-02\n  5.24251610e-02 -5.80032803e-02  6.27384335e-02  7.07182586e-02\n  7.58927241e-02 -3.77486087e-02  6.25816658e-02 -2.89712604e-02\n -6.31601661e-02  5.25093498e-03 -2.71042902e-02  1.70817915e-02\n -2.74686757e-02  1.22844027e-02  6.27805963e-02 -3.76021257e-03\n -3.06509584e-02 -8.06256458e-02 -6.05240129e-02 -9.11984369e-02\n -3.79584194e-03  4.84937429e-02 -1.06186241e-01  6.21189140e-02\n -6.36975318e-02 -6.17115386e-02 -3.23342308e-02 -4.53457311e-02\n  9.35970247e-02 -2.03491119e-03 -3.00037973e-02  6.77352995e-02\n -3.59036494e-03  5.61866388e-02 -7.86416456e-02 -3.03760916e-02\n -5.51747456e-02 -2.84500932e-03  5.76452762e-02  8.35586712e-02\n  3.28903943e-02 -3.76260690e-02  3.19440104e-02 -2.88606398e-02\n  2.31451020e-02 -7.61716664e-02 -2.25486774e-02  3.93390097e-02\n  1.79663356e-02 -5.02210855e-02  7.15343952e-02 -2.77963020e-02\n  2.28058603e-02 -9.89271887e-03  4.51006927e-02 -4.37917523e-02\n  5.96260503e-02 -5.85653335e-02 -2.14698892e-02  2.71938946e-02\n  4.33020256e-02 -9.26014967e-03 -4.94828746e-02 -1.35196708e-02\n -2.04484705e-02 -6.70249686e-02 -7.84093961e-02 -2.38737781e-02\n -2.14124639e-02 -1.63304491e-03  5.26980162e-02  1.16058057e-02\n -5.49360402e-02 -5.87350652e-02 -5.02186045e-02 -5.05788773e-02\n  1.22449443e-01  5.67131564e-02 -4.17637937e-02 -2.12507416e-03\n -5.30138277e-02  4.33651358e-02 -5.57898683e-03 -1.22109596e-02\n -3.76172550e-02 -5.22974804e-02 -1.73301976e-02 -3.68692912e-02\n  6.19046912e-02 -2.43479833e-02 -6.23602457e-02 -6.62348866e-02\n  2.12905183e-03  2.48993058e-02  2.77591534e-02  5.47364950e-02\n  2.26699524e-02 -1.15010645e-02 -1.57651510e-02 -1.50593430e-01\n  3.94990062e-03 -4.19125892e-03 -7.88501576e-02  5.50622419e-02\n -5.91408610e-02 -1.67510062e-02  3.34047452e-02 -1.71639547e-02\n  3.49415988e-02  9.29607004e-02 -2.71553616e-03 -3.37317027e-02\n -6.21086136e-02  3.12881917e-02  6.76379278e-02  3.51435542e-02\n -3.43048871e-02 -3.04443575e-03 -5.03069833e-02 -2.44990587e-02\n -6.24747686e-02 -5.31758368e-03 -4.52208072e-02  3.86059582e-02\n -4.49212082e-02  1.51052803e-01  6.81616887e-02 -3.04204617e-02\n -2.17466820e-02  7.72599084e-03  4.43496462e-03  2.51911610e-04\n -1.00844376e-01  8.54889378e-02 -5.99280149e-02 -1.51736652e-02\n  8.76769722e-02  2.24729646e-02 -6.93258196e-02  7.34177232e-02\n -1.38558568e-02  1.26316324e-02 -5.56351766e-02  1.13625817e-01\n -8.02539811e-02 -3.16032842e-02  3.30004580e-02 -6.90017864e-02\n -2.63925586e-02  2.35942905e-04 -9.07148886e-03  4.24746834e-02\n -9.55128111e-03 -2.77057495e-02 -2.73096859e-02 -1.07106209e-01\n  3.93927917e-02  7.42376689e-03  3.99514809e-02 -7.09020495e-02\n -2.19345298e-02  6.02843948e-02  4.98358756e-02 -1.46392453e-03\n  5.90464240e-03  8.69087316e-03  6.55696820e-03 -2.36159861e-02\n -2.70019472e-02 -2.28817016e-02  3.11406180e-02 -7.30988011e-02\n  6.03520237e-02  6.09903075e-02  5.60276099e-02 -7.34301284e-03\n -5.26567362e-02 -2.65299696e-02 -4.42417860e-02  1.88345909e-02\n -8.14954489e-02  2.27066819e-02  5.70030771e-02  1.47648510e-02\n -6.89939922e-03 -4.07822803e-02 -1.10954093e-02  3.90298553e-02\n -4.85408083e-02  4.09135073e-02 -6.54671937e-02  7.35740140e-02\n  1.26911681e-02  3.00136749e-02 -3.28518786e-02 -1.62071750e-01\n -8.94804895e-02  4.82058562e-02 -4.94790003e-02 -9.87782516e-03\n  6.41239285e-02  7.12786764e-02 -4.26377766e-02 -2.01641601e-02\n -3.46193537e-02 -3.01344134e-02  3.33603360e-02  6.18965439e-02\n -3.34449746e-02 -2.21816208e-02  3.52119915e-02  4.78446297e-02\n -4.51855101e-02 -1.45773515e-02 -9.82143581e-02 -2.03403123e-02\n -3.15992773e-04  1.53243393e-01  1.03172446e-02  4.95225145e-03\n  5.42457961e-02  4.29267474e-02  4.43067141e-02  7.05068260e-02\n  8.04089848e-03 -1.34912869e-02 -4.28665206e-02  1.10032655e-01\n -1.70909590e-03  5.58955334e-02  7.19853938e-02  2.47615092e-02\n  3.52251902e-02  6.65253848e-02  1.21832872e-02  1.22617386e-01\n  8.56769159e-02 -1.53269293e-02 -3.96593474e-02  7.38009736e-02\n -2.56887749e-02 -7.13326782e-02 -9.28727817e-03  5.13379136e-03\n  5.39278518e-03 -6.30126968e-02 -3.34522724e-02  7.17808902e-02\n  6.52781352e-02  3.46691459e-02 -5.86980842e-02 -7.76937306e-02\n  1.02605727e-02 -2.96042915e-02 -9.76502057e-03  1.48929656e-02\n -7.48183532e-03  6.92784041e-02 -6.42380565e-02  1.02737760e-02]",
                  "frontend-1"
                ],
                [
                  "1",
                  "2022-05-02 16:00:00",
                  "[-0.04647663  0.02830305  0.01143864  0.01614726 -0.00970807  0.03026093\n  0.06236859 -0.04190517 -0.02422191  0.02195217  0.03171324 -0.03253515\n -0.03232237  0.01109739  0.00660856 -0.02495736  0.01352507  0.10310125\n -0.04042675  0.03155486  0.02602524 -0.05793325 -0.01240604 -0.07247389\n -0.03895773 -0.00855103  0.03296356  0.09509028 -0.07376587 -0.12925188\n -0.03112545  0.03336009  0.03442385  0.01407951  0.07618719 -0.03975154\n -0.00672386  0.02406004 -0.00824712  0.05948218  0.00914595 -0.04009708\n  0.02625319 -0.0322446   0.00214244 -0.02272611 -0.06656493 -0.04693651\n  0.06053873 -0.02056564 -0.04100591 -0.01765252  0.06579807  0.01681889\n  0.01086693  0.05647837  0.05448199  0.06400651  0.05413171  0.06500606\n  0.02164502 -0.00521346 -0.14913805  0.11114928  0.05646056  0.02102741\n -0.01243535  0.015601   -0.05116261  0.00355279 -0.0530783   0.01092384\n -0.03612385  0.08812923 -0.07387863  0.06463094  0.01515501  0.05598377\n -0.04866019 -0.03454011 -0.01707125 -0.04161822 -0.02522409  0.02347891\n  0.00641125 -0.03349691  0.00565978 -0.00665262 -0.01160694 -0.04244386\n -0.02285473 -0.04606145  0.00095298 -0.03496142 -0.0176612  -0.04227129\n  0.08781833  0.01512515  0.02921839  0.07582623 -0.00737157  0.05244033\n  0.02804687 -0.07233372  0.01098387  0.00524565  0.02109779  0.03418954\n  0.0386987  -0.01240836 -0.07744309 -0.05700747  0.01686947 -0.02293604\n  0.02780307  0.05501199 -0.04493883 -0.03448432 -0.01960094  0.04096194\n  0.06259088  0.04222018  0.01883115 -0.01173127  0.02688715 -0.07229213\n  0.1134164   0.0894756   0.076896   -0.01686976  0.09427541 -0.0442519\n -0.04693779  0.00298823  0.01153529 -0.04986451 -0.02363235  0.02261775\n  0.06351173 -0.00476576 -0.03967644 -0.03770769 -0.02763139 -0.09428004\n  0.01902003  0.07321899 -0.08999734  0.02290457 -0.06564093 -0.09034872\n -0.04151119  0.00175192  0.06531078 -0.03648445 -0.02543646  0.01248455\n -0.00632144  0.09355205 -0.01477071 -0.01503373 -0.11123517  0.02284986\n  0.04388237  0.1326491   0.05824563 -0.09509029  0.08070187  0.01032975\n  0.02413198 -0.05206443  0.00127457  0.0118291   0.02081019 -0.01897752\n  0.09953167 -0.01585175 -0.03341433 -0.03045075  0.01537806 -0.05123228\n  0.07231974 -0.0195712  -0.02014034  0.01743027  0.05952303 -0.03883618\n -0.03201115 -0.12031043 -0.02860031 -0.03780661 -0.05207199 -0.00594612\n -0.02857485  0.04047511  0.03212047 -0.01216691 -0.01559655 -0.03107141\n -0.03682286 -0.04763684  0.08659733  0.07911795 -0.0751989   0.02382726\n -0.1259501   0.04850528 -0.02686526 -0.00774244 -0.07837012 -0.05774445\n -0.03076069  0.03205043  0.02105737 -0.00331415 -0.01597621  0.00674217\n  0.00366172  0.01583632  0.05436448  0.02528126 -0.03325155  0.02068354\n -0.03989016 -0.12785472  0.00684064 -0.02300643 -0.05382296  0.05393098\n  0.01533874 -0.04348257  0.01439214 -0.08972812  0.02382171  0.12849842\n -0.0539727  -0.00525539  0.05584135  0.0152339   0.09063318 -0.04110081\n  0.0056143   0.08394757 -0.01088703 -0.04079162 -0.08422897 -0.04996426\n -0.07672767  0.09984027 -0.03479588  0.11241908  0.08820254  0.01000739\n -0.02109333 -0.00204015  0.00536434  0.02171954 -0.08631555  0.1038858\n -0.03657385  0.00311942 -0.01801506  0.06467665 -0.05868378  0.05045393\n -0.00395636  0.0015017  -0.11137553  0.11980433 -0.05469218 -0.06014764\n  0.02901755 -0.03858192 -0.01691346  0.02188832 -0.02671488  0.03779502\n -0.06419491 -0.05886261 -0.04816174 -0.03687069  0.057905   -0.00705206\n -0.01499666 -0.05412325 -0.05799342  0.03720921  0.01100394 -0.00832216\n  0.02302448 -0.01514934 -0.00228756 -0.00020053 -0.03188351 -0.00522536\n  0.03034225 -0.06403026  0.07221207  0.04554195  0.02490319  0.03594822\n -0.01173408 -0.00251194 -0.04308586 -0.04758139 -0.10344931  0.03325866\n  0.0272831  -0.01173071 -0.00383377 -0.03051091 -0.0465185   0.04021972\n -0.02183102  0.05399926 -0.02891849  0.0400795   0.04129986 -0.02157372\n -0.05837762 -0.14579858 -0.100399    0.01809089  0.01220921  0.0182423\n  0.0156645   0.05109844  0.02855922 -0.01343152 -0.04436696  0.03158398\n  0.00385707  0.06927808  0.00110231 -0.04096875  0.06394355  0.09336405\n -0.00490498 -0.03342597 -0.12897971  0.00256702  0.04564443  0.16373983\n  0.06888118 -0.025973    0.02484605  0.02030281 -0.0262316   0.03485654\n  0.05206127 -0.02609648 -0.05305433  0.0950488  -0.0261822   0.05145767\n  0.07919875 -0.01414735  0.00924525  0.02935334 -0.00411927  0.06243464\n  0.08882754 -0.05179696 -0.08619701  0.08302831 -0.01314258 -0.0655484\n -0.01879233  0.00899057 -0.03979553 -0.04593416 -0.02290672  0.04918868\n  0.0383471   0.03433361 -0.03649819 -0.0363334   0.0172629  -0.03955946\n -0.00717573  0.04271835 -0.05052109  0.02772216 -0.0366088   0.04899495]",
                  "cartservice-2"
                ],
                [
                  "2",
                  "2022-05-02 16:00:00",
                  "[-8.47293437e-02  2.86834035e-02  3.66072133e-02 -1.89151820e-02\n  1.75062157e-02  3.61061692e-02  5.05068786e-02 -6.46922141e-02\n  2.04279367e-03  3.27486023e-02  5.84862046e-02 -2.55778655e-02\n -4.95859943e-02  1.78921279e-02  4.20495160e-02 -2.64966208e-02\n -5.98261617e-02  4.15123068e-02 -3.01737469e-02  7.85593241e-02\n  7.89005831e-02 -4.62936088e-02  2.45531127e-02 -7.52634108e-02\n -4.08124216e-02  1.48131065e-02  6.52915537e-02  5.41619919e-02\n -5.03695235e-02 -1.38547301e-01 -2.92356797e-02 -1.77404634e-03\n  1.69249102e-02 -3.91338430e-02  9.90180373e-02 -5.89394532e-02\n  1.20134447e-02 -4.75430395e-03 -2.03902144e-02  5.93267381e-02\n -3.60856503e-02 -1.04936780e-02  3.65169570e-02 -7.99666941e-02\n  2.23563146e-02 -3.89628746e-02 -6.37656748e-02 -3.61666270e-02\n  3.47482674e-02  2.73568146e-02 -5.59623018e-02  1.90462917e-02\n  2.69239582e-02  6.62266091e-02  2.21970100e-02  6.13222830e-02\n  6.04924709e-02  8.22240673e-03  4.74181101e-02  1.05678402e-01\n  1.53752984e-02  9.59481671e-03 -1.26175568e-01  1.07059218e-01\n  1.04149342e-01  6.52688816e-02  6.36015669e-04  2.88190134e-02\n  5.26525686e-03 -5.58259059e-03 -2.44751964e-02  3.71167883e-02\n -1.13180764e-02  4.05846909e-02 -3.67198326e-02  8.61145407e-02\n  3.21832858e-02  6.69249073e-02 -3.64992470e-02 -1.41840065e-02\n -2.81641353e-02 -2.48746146e-02 -4.52814549e-02  2.16811877e-02\n -8.94226425e-04 -1.51539054e-02  9.54406150e-03 -3.55994590e-02\n -1.34620583e-02 -3.19060124e-02 -7.10029528e-02 -2.89763678e-02\n -2.51038256e-03 -5.83785586e-02 -1.47074228e-02 -2.15667095e-02\n  6.62109181e-02  2.13770079e-03 -1.45441806e-03  6.33460507e-02\n  1.16411429e-02 -6.99252542e-03 -1.10042337e-02 -5.48089482e-02\n -3.26320007e-02 -2.20951773e-02  3.14658917e-02 -1.28364572e-02\n  7.45787621e-02 -3.02621219e-02 -4.80083376e-02 -2.29650605e-02\n -3.07185086e-03 -5.18885395e-03  5.61844856e-02  1.58049613e-02\n -2.28537209e-02 -4.78759743e-02 -3.89058851e-02  3.24363746e-02\n  5.63522130e-02  1.95528474e-02 -3.88373109e-03 -3.42046507e-02\n  3.60253565e-02 -5.62699474e-02  4.49440107e-02  4.07896191e-02\n  8.78338963e-02 -8.51355493e-03  6.36314154e-02 -4.87160357e-03\n -2.63536479e-02 -2.65629758e-04 -3.25491875e-02  6.91169733e-03\n -3.59843709e-02  1.74202509e-02  6.19132668e-02  4.37276391e-03\n -3.58939357e-02 -9.71745551e-02 -1.78423617e-02 -9.64527354e-02\n  8.42813868e-03  9.83287022e-02 -4.76349853e-02  6.00635866e-03\n -1.01059929e-01 -4.17240374e-02 -3.24363448e-02 -2.97570284e-02\n  3.17236013e-03 -2.44496204e-02 -3.91330151e-03  7.12761953e-02\n  1.51713528e-02  5.24981879e-02 -1.60804465e-02 -1.38471518e-02\n -8.91108364e-02 -2.40324102e-02  5.70779890e-02  1.54197529e-01\n  4.34622020e-02 -1.04224876e-01  3.33000571e-02 -2.60949079e-02\n  3.21155339e-02 -7.14949965e-02 -1.62557662e-02  4.40756641e-02\n  1.55452210e-02 -3.43423933e-02  1.13832869e-01 -6.46150932e-02\n -4.88809776e-03 -4.83886898e-02  2.67231911e-02 -5.56410961e-02\n  3.29836905e-02 -7.08718821e-02 -1.94614958e-02  6.21202178e-02\n  8.93563628e-02 -1.27360011e-02 -4.82411236e-02 -3.82025503e-02\n -2.28653457e-02 -1.11609645e-01 -7.75057301e-02 -3.10315266e-02\n -1.21229794e-02  2.30312347e-02  3.89193557e-02 -1.00470809e-02\n -3.78440581e-02 -6.08738437e-02 -9.86178033e-03 -9.28658620e-02\n  9.78106782e-02  5.06202616e-02 -5.56845963e-02  1.39020011e-03\n -5.08465022e-02  1.02501698e-01  1.26960464e-02 -3.58545147e-02\n -5.19774891e-02  1.29234204e-02 -5.14956787e-02 -3.17609981e-02\n -8.04622378e-03 -4.29964624e-02 -3.67876887e-02 -2.31977776e-02\n -1.13401739e-02  2.30976753e-02  2.92093996e-02  1.19500495e-02\n  3.20644453e-02 -6.42447472e-02 -6.22091442e-02 -1.23944744e-01\n -1.61251742e-02 -2.99088210e-02 -3.83887291e-02  5.70165887e-02\n -4.09160629e-02 -4.81508151e-02  9.10012498e-02 -7.36151561e-02\n  4.90048081e-02  7.05300421e-02  1.25279436e-02 -1.73748576e-03\n -7.65498728e-02  1.88734122e-02  3.08125485e-02  1.23354271e-02\n  3.24652046e-02  3.53955701e-02 -7.26613030e-02 -6.10866118e-04\n -8.92144740e-02 -2.64037512e-02 -4.85360511e-02  6.35428876e-02\n -4.11692560e-02  1.42431751e-01  5.21907732e-02  2.98179239e-02\n -2.45135091e-02 -1.31609559e-04  5.92023646e-03  1.11461356e-02\n -7.34856799e-02  9.96215269e-02 -6.75768405e-02 -2.10981779e-02\n  7.32598156e-02 -2.84243897e-02 -5.15898205e-02  7.26790205e-02\n -2.44224574e-02  3.15512232e-02 -6.27474561e-02  1.25304103e-01\n -7.11162314e-02 -1.09314993e-02  3.84142250e-03 -8.08907226e-02\n -2.84860115e-02  1.99508257e-02 -3.10089756e-02  4.06010225e-02\n -3.68822999e-02 -7.70575404e-02 -4.19410840e-02 -6.35439232e-02\n  4.85047065e-02 -5.37895039e-02  1.31424973e-02 -8.01662132e-02\n -2.48272493e-02  5.30533157e-02 -3.53182154e-03  3.37432101e-02\n -3.72191542e-03  2.75320839e-02 -2.11864654e-02 -5.60887828e-02\n -3.67368087e-02 -8.07721913e-03  8.16678163e-03 -2.81027630e-02\n  5.12049869e-02  3.07534039e-02  1.78265776e-02  9.08279698e-03\n -3.55724804e-02 -3.32493484e-02 -3.35519463e-02 -3.68634202e-02\n -8.29534605e-02 -1.51376836e-02  5.73891178e-02  1.26947248e-02\n -2.58291736e-02 -4.39476781e-02  6.27025738e-02 -6.20390987e-03\n -5.81718534e-02  6.02627322e-02 -3.62569839e-02  4.13295254e-02\n  5.14829569e-02  2.11897735e-02  4.65230690e-03 -1.33590296e-01\n -2.49496643e-02  3.61373089e-02 -3.10274530e-02  1.00855343e-02\n  8.73220246e-03  4.14681770e-02 -2.77651306e-02 -2.54333951e-02\n -2.45020427e-02  4.33811210e-02  3.98121998e-02  8.76693204e-02\n -4.77056801e-02  3.89193487e-03  9.41868201e-02  3.57615203e-02\n -4.00110930e-02  8.28447402e-04 -7.31927678e-02  8.31988081e-03\n  5.17764911e-02  1.82945997e-01  1.02670696e-02 -2.38627591e-03\n  3.25471312e-02  1.50132226e-02  2.85582785e-02  3.82651202e-02\n  6.00019582e-02  2.30267625e-02 -5.76962829e-02  1.12876333e-01\n -3.78305018e-02  4.00264673e-02  7.57482350e-02  1.17241032e-02\n -2.04937242e-04  6.53348863e-02 -2.50079241e-02  1.26515433e-01\n  5.22840135e-02 -5.94373383e-02 -1.66864581e-02  1.29068643e-01\n -2.99622249e-02 -5.80024906e-02 -2.92412043e-02  6.94551878e-03\n  2.15140674e-02 -5.93115948e-02  7.23783858e-03  9.95524749e-02\n  4.24416512e-02  7.71097466e-02 -5.21101207e-02 -3.79853845e-02\n  2.10591443e-02 -1.72265824e-02  1.14751922e-03 -3.55866179e-03\n  1.53261132e-03  3.87751870e-02 -4.45550010e-02 -1.16388500e-03]",
                  "cartservice-2"
                ],
                [
                  "3",
                  "2022-05-02 16:00:00",
                  "[-6.35791346e-02  2.52424423e-02  9.64165607e-04  1.09428614e-02\n -1.74116641e-02  2.21170969e-02  3.71761732e-02 -5.78494966e-02\n -2.56889611e-02  4.46208529e-02  1.46637065e-02 -4.78167236e-02\n -5.68985716e-02  3.23046953e-03  5.86386584e-03  1.16666872e-03\n -4.49875183e-02  7.13139623e-02 -4.38685752e-02  2.97619682e-02\n  8.33054483e-02 -3.70620154e-02  3.62082459e-02 -7.23683462e-02\n -2.48778071e-02  1.40085388e-02  3.58939022e-02  1.14049226e-01\n -4.23620641e-02 -1.42290756e-01 -4.65645790e-02  4.74142246e-02\n  5.24921454e-02  1.72816552e-02  7.79993236e-02 -4.41500545e-02\n  2.16573849e-02  3.12985815e-02 -9.50408913e-03  1.02155089e-01\n -6.83483928e-02 -2.51148045e-02  3.67640033e-02 -6.99656829e-02\n -7.01535610e-04 -5.85678928e-02 -7.70160779e-02 -5.56994602e-02\n  6.30851388e-02  1.05079841e-02 -3.76255959e-02 -3.22989491e-03\n  2.20777821e-02  8.86140093e-02  5.32991737e-02  7.38029853e-02\n  4.68412563e-02  4.30133604e-02  5.89398518e-02  1.20378986e-01\n  2.31039394e-02  2.55097933e-02 -1.44372702e-01  1.26368523e-01\n  5.88675104e-02  5.61676249e-02 -2.95832362e-02  3.19230221e-02\n -3.21508981e-02  1.60018019e-02 -1.57738626e-02  4.55737785e-02\n -4.78163511e-02  6.09336421e-02 -7.87005424e-02  5.98343387e-02\n -2.12286995e-03  5.87282218e-02 -4.24490348e-02 -2.48604678e-02\n -1.54030714e-02 -5.12007438e-02 -1.85910799e-02  5.65974647e-03\n -2.84921937e-02 -2.66908389e-02 -1.16269814e-03  1.45243686e-02\n  8.49426724e-03 -7.82708544e-03 -2.20702048e-02 -6.72406703e-02\n -1.69022325e-02 -5.75075783e-02 -4.18862849e-02 -2.09567938e-02\n  5.45339584e-02  1.43509787e-02  1.71779767e-02  6.19454570e-02\n  5.85808791e-02  2.55183987e-02 -2.46795341e-02 -8.92096832e-02\n -7.47144781e-03 -1.91807579e-02  4.31863964e-02  1.05972020e-02\n  7.09672943e-02  6.00394793e-04 -8.71752203e-02 -4.58694138e-02\n  1.31952437e-02 -1.16894143e-02  6.77547380e-02  3.87960374e-02\n -1.24757271e-02 -3.09886020e-02 -2.52179969e-02  3.42721343e-02\n  3.13020162e-02  4.27510105e-02 -6.72647171e-03 -4.11454327e-02\n  2.50214189e-02 -8.43633935e-02  1.04764946e-01  8.38692561e-02\n  9.25897434e-02 -2.95035429e-02  1.13943473e-01 -3.67403328e-02\n -4.16600704e-02 -7.40734069e-03 -1.97024103e-02 -6.30475406e-04\n -3.61253731e-02 -2.93597169e-02  5.25257848e-02 -1.37016494e-02\n -2.76398603e-02 -7.65964165e-02 -1.47150010e-02 -9.85254124e-02\n  3.95508185e-02  8.22234377e-02 -3.81135307e-02  5.58556654e-02\n -7.76678175e-02 -6.62307516e-02 -3.01206787e-03  1.09544932e-03\n  6.13415875e-02 -2.05951706e-02 -1.96496230e-02 -1.24023827e-02\n -8.85201339e-03  8.64210129e-02 -7.74011621e-03 -1.22073954e-02\n -6.98293000e-02 -5.45209870e-02  5.04133627e-02  1.36207834e-01\n  3.95748429e-02 -8.26853588e-02  6.25527948e-02 -1.79868899e-02\n  1.65935978e-02 -6.71094954e-02  1.49394386e-02  2.09233742e-02\n  2.00623721e-02 -3.70667838e-02  8.64145085e-02 -4.58690040e-02\n -2.61516985e-03 -2.26250440e-02 -1.07154588e-03 -3.89783792e-02\n  2.43693460e-02  3.37644853e-03 -4.68572741e-03  2.66063102e-02\n  1.83409695e-02  3.78282188e-04 -2.67588571e-02 -8.68806094e-02\n -3.35045122e-02 -5.74810579e-02 -6.19234443e-02 -4.79096686e-03\n -2.44792849e-02  3.20535935e-02  3.04973014e-02 -2.62100678e-02\n  7.34915864e-03 -7.53848553e-02 -4.75324355e-02 -6.80157542e-02\n  9.24722701e-02  4.86989096e-02 -5.06960452e-02  1.61744878e-02\n -9.68991667e-02  6.94404840e-02 -1.86297912e-02 -4.51059975e-02\n -3.71135436e-02  2.94233449e-02 -5.19517250e-02 -3.60129438e-02\n  1.36720100e-02 -5.03088273e-02 -2.16194633e-02 -1.90457311e-02\n  2.94642616e-02 -5.72446454e-03  1.20265866e-02  2.15895791e-02\n  2.26267073e-02 -2.71694679e-02 -6.44448325e-02 -1.48449957e-01\n -1.67244785e-02 -4.11235429e-02 -2.41464060e-02  4.15015854e-02\n -2.02523768e-02 -3.63178328e-02  4.48793033e-03 -9.71799642e-02\n  3.27321813e-02  9.14216787e-02  4.01001947e-04 -4.65683974e-02\n -1.35682598e-02  4.15968969e-02  3.66379693e-02 -1.03315450e-02\n  2.67661153e-03  5.52458614e-02 -6.97788522e-02 -2.84469454e-03\n -6.30083904e-02 -6.96960138e-03 -3.99429426e-02  6.76628202e-02\n -3.45974639e-02  9.52399895e-02  3.81226093e-02 -1.35744177e-02\n -4.15852666e-02  2.18130071e-02  7.44708441e-03  1.03847040e-02\n -1.07658878e-01  1.14903793e-01 -2.12824810e-02 -1.60408039e-02\n  7.46496320e-02  3.22215147e-02 -3.66285555e-02  6.56295046e-02\n  1.92776881e-02 -2.04145792e-03 -7.85943493e-02  1.28216237e-01\n -2.77577974e-02  4.01059771e-03  2.09427048e-02 -5.24175912e-02\n -2.28544772e-02 -6.63410313e-03 -2.86920965e-02  2.01703757e-02\n -2.04151943e-02 -6.26413599e-02 -4.49411608e-02 -4.49745022e-02\n  2.86939703e-02 -5.99963441e-02  3.66446562e-02 -4.79569435e-02\n -1.45530989e-02  2.75058001e-02 -9.61505901e-03  3.97197902e-03\n  7.61420652e-03 -1.19563742e-02 -6.27331436e-04 -3.47543284e-02\n -2.64022667e-02 -3.07486560e-02  2.19456926e-02 -6.83303997e-02\n  7.93731064e-02  5.05024232e-02  2.09906213e-02  9.82836541e-03\n -2.01123417e-03 -3.87645140e-02 -2.23225933e-02 -1.36797978e-02\n -9.64371189e-02  5.35245463e-02  4.04826850e-02  2.64602806e-02\n  1.02936970e-02 -4.79502492e-02 -2.05557048e-02  3.80296297e-02\n -1.34025817e-04  4.05194387e-02 -7.87833035e-02  8.33587125e-02\n  9.23628658e-02  1.82670858e-02 -7.69534260e-02 -1.68717012e-01\n -6.72766492e-02 -3.15792789e-03 -6.37041917e-03 -2.45365221e-03\n  5.47601990e-02  6.72121495e-02 -2.49995608e-02 -3.13176624e-02\n -1.57665331e-02 -2.12260219e-03  9.45651159e-03  8.37260261e-02\n -4.40551247e-03 -1.44067444e-02  5.42295389e-02  3.43309082e-02\n -2.72091404e-02 -3.09323054e-02 -7.98388794e-02 -2.12927982e-02\n  3.09735220e-02  1.51965871e-01  6.59538852e-03  2.68659573e-02\n  4.05225046e-02  2.29624417e-02  1.18337274e-02  6.81552440e-02\n  1.97670963e-02 -1.15909362e-02 -6.28069565e-02  1.19498037e-01\n -3.33373472e-02  6.76074177e-02  7.58963078e-02  2.32368335e-02\n  2.72313431e-02  8.36615711e-02 -3.26481485e-03  7.40402341e-02\n  8.16016570e-02 -3.37923579e-02 -5.83120212e-02  1.23934112e-01\n  2.01134775e-02 -7.26898611e-02 -3.99039760e-02 -2.09294036e-02\n -2.39735246e-02 -8.85003135e-02 -2.83664074e-02  3.25018875e-02\n  2.11021136e-02  3.64130065e-02 -4.63235751e-02 -4.95050661e-02\n  9.14117787e-03 -3.26981656e-02  1.42421117e-02  2.37562358e-02\n -2.29766332e-02  5.08122891e-02 -4.93027680e-02  2.65185349e-02]",
                  "cartservice-2"
                ],
                [
                  "4",
                  "2022-05-02 16:00:00",
                  "[-8.47293437e-02  2.86834035e-02  3.66072133e-02 -1.89151820e-02\n  1.75062157e-02  3.61061692e-02  5.05068786e-02 -6.46922141e-02\n  2.04279367e-03  3.27486023e-02  5.84862046e-02 -2.55778655e-02\n -4.95859943e-02  1.78921279e-02  4.20495160e-02 -2.64966208e-02\n -5.98261617e-02  4.15123068e-02 -3.01737469e-02  7.85593241e-02\n  7.89005831e-02 -4.62936088e-02  2.45531127e-02 -7.52634108e-02\n -4.08124216e-02  1.48131065e-02  6.52915537e-02  5.41619919e-02\n -5.03695235e-02 -1.38547301e-01 -2.92356797e-02 -1.77404634e-03\n  1.69249102e-02 -3.91338430e-02  9.90180373e-02 -5.89394532e-02\n  1.20134447e-02 -4.75430395e-03 -2.03902144e-02  5.93267381e-02\n -3.60856503e-02 -1.04936780e-02  3.65169570e-02 -7.99666941e-02\n  2.23563146e-02 -3.89628746e-02 -6.37656748e-02 -3.61666270e-02\n  3.47482674e-02  2.73568146e-02 -5.59623018e-02  1.90462917e-02\n  2.69239582e-02  6.62266091e-02  2.21970100e-02  6.13222830e-02\n  6.04924709e-02  8.22240673e-03  4.74181101e-02  1.05678402e-01\n  1.53752984e-02  9.59481671e-03 -1.26175568e-01  1.07059218e-01\n  1.04149342e-01  6.52688816e-02  6.36015669e-04  2.88190134e-02\n  5.26525686e-03 -5.58259059e-03 -2.44751964e-02  3.71167883e-02\n -1.13180764e-02  4.05846909e-02 -3.67198326e-02  8.61145407e-02\n  3.21832858e-02  6.69249073e-02 -3.64992470e-02 -1.41840065e-02\n -2.81641353e-02 -2.48746146e-02 -4.52814549e-02  2.16811877e-02\n -8.94226425e-04 -1.51539054e-02  9.54406150e-03 -3.55994590e-02\n -1.34620583e-02 -3.19060124e-02 -7.10029528e-02 -2.89763678e-02\n -2.51038256e-03 -5.83785586e-02 -1.47074228e-02 -2.15667095e-02\n  6.62109181e-02  2.13770079e-03 -1.45441806e-03  6.33460507e-02\n  1.16411429e-02 -6.99252542e-03 -1.10042337e-02 -5.48089482e-02\n -3.26320007e-02 -2.20951773e-02  3.14658917e-02 -1.28364572e-02\n  7.45787621e-02 -3.02621219e-02 -4.80083376e-02 -2.29650605e-02\n -3.07185086e-03 -5.18885395e-03  5.61844856e-02  1.58049613e-02\n -2.28537209e-02 -4.78759743e-02 -3.89058851e-02  3.24363746e-02\n  5.63522130e-02  1.95528474e-02 -3.88373109e-03 -3.42046507e-02\n  3.60253565e-02 -5.62699474e-02  4.49440107e-02  4.07896191e-02\n  8.78338963e-02 -8.51355493e-03  6.36314154e-02 -4.87160357e-03\n -2.63536479e-02 -2.65629758e-04 -3.25491875e-02  6.91169733e-03\n -3.59843709e-02  1.74202509e-02  6.19132668e-02  4.37276391e-03\n -3.58939357e-02 -9.71745551e-02 -1.78423617e-02 -9.64527354e-02\n  8.42813868e-03  9.83287022e-02 -4.76349853e-02  6.00635866e-03\n -1.01059929e-01 -4.17240374e-02 -3.24363448e-02 -2.97570284e-02\n  3.17236013e-03 -2.44496204e-02 -3.91330151e-03  7.12761953e-02\n  1.51713528e-02  5.24981879e-02 -1.60804465e-02 -1.38471518e-02\n -8.91108364e-02 -2.40324102e-02  5.70779890e-02  1.54197529e-01\n  4.34622020e-02 -1.04224876e-01  3.33000571e-02 -2.60949079e-02\n  3.21155339e-02 -7.14949965e-02 -1.62557662e-02  4.40756641e-02\n  1.55452210e-02 -3.43423933e-02  1.13832869e-01 -6.46150932e-02\n -4.88809776e-03 -4.83886898e-02  2.67231911e-02 -5.56410961e-02\n  3.29836905e-02 -7.08718821e-02 -1.94614958e-02  6.21202178e-02\n  8.93563628e-02 -1.27360011e-02 -4.82411236e-02 -3.82025503e-02\n -2.28653457e-02 -1.11609645e-01 -7.75057301e-02 -3.10315266e-02\n -1.21229794e-02  2.30312347e-02  3.89193557e-02 -1.00470809e-02\n -3.78440581e-02 -6.08738437e-02 -9.86178033e-03 -9.28658620e-02\n  9.78106782e-02  5.06202616e-02 -5.56845963e-02  1.39020011e-03\n -5.08465022e-02  1.02501698e-01  1.26960464e-02 -3.58545147e-02\n -5.19774891e-02  1.29234204e-02 -5.14956787e-02 -3.17609981e-02\n -8.04622378e-03 -4.29964624e-02 -3.67876887e-02 -2.31977776e-02\n -1.13401739e-02  2.30976753e-02  2.92093996e-02  1.19500495e-02\n  3.20644453e-02 -6.42447472e-02 -6.22091442e-02 -1.23944744e-01\n -1.61251742e-02 -2.99088210e-02 -3.83887291e-02  5.70165887e-02\n -4.09160629e-02 -4.81508151e-02  9.10012498e-02 -7.36151561e-02\n  4.90048081e-02  7.05300421e-02  1.25279436e-02 -1.73748576e-03\n -7.65498728e-02  1.88734122e-02  3.08125485e-02  1.23354271e-02\n  3.24652046e-02  3.53955701e-02 -7.26613030e-02 -6.10866118e-04\n -8.92144740e-02 -2.64037512e-02 -4.85360511e-02  6.35428876e-02\n -4.11692560e-02  1.42431751e-01  5.21907732e-02  2.98179239e-02\n -2.45135091e-02 -1.31609559e-04  5.92023646e-03  1.11461356e-02\n -7.34856799e-02  9.96215269e-02 -6.75768405e-02 -2.10981779e-02\n  7.32598156e-02 -2.84243897e-02 -5.15898205e-02  7.26790205e-02\n -2.44224574e-02  3.15512232e-02 -6.27474561e-02  1.25304103e-01\n -7.11162314e-02 -1.09314993e-02  3.84142250e-03 -8.08907226e-02\n -2.84860115e-02  1.99508257e-02 -3.10089756e-02  4.06010225e-02\n -3.68822999e-02 -7.70575404e-02 -4.19410840e-02 -6.35439232e-02\n  4.85047065e-02 -5.37895039e-02  1.31424973e-02 -8.01662132e-02\n -2.48272493e-02  5.30533157e-02 -3.53182154e-03  3.37432101e-02\n -3.72191542e-03  2.75320839e-02 -2.11864654e-02 -5.60887828e-02\n -3.67368087e-02 -8.07721913e-03  8.16678163e-03 -2.81027630e-02\n  5.12049869e-02  3.07534039e-02  1.78265776e-02  9.08279698e-03\n -3.55724804e-02 -3.32493484e-02 -3.35519463e-02 -3.68634202e-02\n -8.29534605e-02 -1.51376836e-02  5.73891178e-02  1.26947248e-02\n -2.58291736e-02 -4.39476781e-02  6.27025738e-02 -6.20390987e-03\n -5.81718534e-02  6.02627322e-02 -3.62569839e-02  4.13295254e-02\n  5.14829569e-02  2.11897735e-02  4.65230690e-03 -1.33590296e-01\n -2.49496643e-02  3.61373089e-02 -3.10274530e-02  1.00855343e-02\n  8.73220246e-03  4.14681770e-02 -2.77651306e-02 -2.54333951e-02\n -2.45020427e-02  4.33811210e-02  3.98121998e-02  8.76693204e-02\n -4.77056801e-02  3.89193487e-03  9.41868201e-02  3.57615203e-02\n -4.00110930e-02  8.28447402e-04 -7.31927678e-02  8.31988081e-03\n  5.17764911e-02  1.82945997e-01  1.02670696e-02 -2.38627591e-03\n  3.25471312e-02  1.50132226e-02  2.85582785e-02  3.82651202e-02\n  6.00019582e-02  2.30267625e-02 -5.76962829e-02  1.12876333e-01\n -3.78305018e-02  4.00264673e-02  7.57482350e-02  1.17241032e-02\n -2.04937242e-04  6.53348863e-02 -2.50079241e-02  1.26515433e-01\n  5.22840135e-02 -5.94373383e-02 -1.66864581e-02  1.29068643e-01\n -2.99622249e-02 -5.80024906e-02 -2.92412043e-02  6.94551878e-03\n  2.15140674e-02 -5.93115948e-02  7.23783858e-03  9.95524749e-02\n  4.24416512e-02  7.71097466e-02 -5.21101207e-02 -3.79853845e-02\n  2.10591443e-02 -1.72265824e-02  1.14751922e-03 -3.55866179e-03\n  1.53261132e-03  3.87751870e-02 -4.45550010e-02 -1.16388500e-03]",
                  "cartservice-2"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>embedding</th>\n",
              "      <th>cmdb_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-05-02 16:00:00</td>\n",
              "      <td>[-0.06920783, 0.077275746, 0.019308727, 0.0091...</td>\n",
              "      <td>frontend-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-05-02 16:00:00</td>\n",
              "      <td>[-0.04647663, 0.028303048, 0.011438644, 0.0161...</td>\n",
              "      <td>cartservice-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-05-02 16:00:00</td>\n",
              "      <td>[-0.08472934, 0.028683404, 0.036607213, -0.018...</td>\n",
              "      <td>cartservice-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-05-02 16:00:00</td>\n",
              "      <td>[-0.063579135, 0.025242442, 0.0009641656, 0.01...</td>\n",
              "      <td>cartservice-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-05-02 16:00:00</td>\n",
              "      <td>[-0.08472934, 0.028683404, 0.036607213, -0.018...</td>\n",
              "      <td>cartservice-2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             datetime                                          embedding  \\\n",
              "0 2022-05-02 16:00:00  [-0.06920783, 0.077275746, 0.019308727, 0.0091...   \n",
              "1 2022-05-02 16:00:00  [-0.04647663, 0.028303048, 0.011438644, 0.0161...   \n",
              "2 2022-05-02 16:00:00  [-0.08472934, 0.028683404, 0.036607213, -0.018...   \n",
              "3 2022-05-02 16:00:00  [-0.063579135, 0.025242442, 0.0009641656, 0.01...   \n",
              "4 2022-05-02 16:00:00  [-0.08472934, 0.028683404, 0.036607213, -0.018...   \n",
              "\n",
              "         cmdb_id  \n",
              "0     frontend-1  \n",
              "1  cartservice-2  \n",
              "2  cartservice-2  \n",
              "3  cartservice-2  \n",
              "4  cartservice-2  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, hashlib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# embeddings cache for unique normalized strings\n",
        "CACHE_PATH = \"/Users/arinagoncharova/Documents/diploma/repo/data/AIOps/preprocessed_while_modelling/e5_small_embeddings/embeddings_cache.parquet\"\n",
        "\n",
        "def norm_hash(s: str) -> str:\n",
        "    import hashlib\n",
        "    return hashlib.sha1(s.encode('utf-8')).hexdigest()\n",
        "\n",
        "def load_cache():\n",
        "    import pandas as pd, os\n",
        "    if os.path.exists(CACHE_PATH):\n",
        "        return pd.read_parquet(CACHE_PATH)\n",
        "    return pd.DataFrame(columns=[\"norm_hash\", \"normalized_value\", \"embedding\"])\n",
        "\n",
        "def save_cache(df_cache):\n",
        "    df_cache = df_cache.drop_duplicates(\"norm_hash\")\n",
        "    df_cache.to_parquet(CACHE_PATH, index=False)\n",
        "\n",
        "def compute_embeddings_with_cache(df_logs: pd.DataFrame, text_col: str = \"value\",\n",
        "                                  model_name: str = \"intfloat/e5-small-v2\",\n",
        "                                  batch_size: int = 16):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    df = df_logs.copy()\n",
        "    df[\"normalized_value\"] = df[text_col].fillna(\"\").map(normalize_log)\n",
        "    df[\"norm_hash\"] = df[\"normalized_value\"].map(norm_hash)\n",
        "    cache = load_cache()\n",
        "    known = set(cache[\"norm_hash\"].tolist())\n",
        "    uniq = df[[\"norm_hash\", \"normalized_value\"]].drop_duplicates()\n",
        "    to_compute = uniq[~uniq[\"norm_hash\"].isin(known)]\n",
        "    if not to_compute.empty:\n",
        "        texts = to_compute[\"normalized_value\"].tolist()\n",
        "        embs = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
        "        add = to_compute.copy().reset_index(drop=True)\n",
        "        add[\"embedding\"] = list(embs)\n",
        "        cache = pd.concat([cache, add[[\"norm_hash\",\"normalized_value\",\"embedding\"]]], ignore_index=True)\n",
        "        save_cache(cache)\n",
        "    df = df.merge(cache[[\"norm_hash\",\"embedding\"]], on=\"norm_hash\", how=\"left\")\n",
        "    return df\n",
        "\n",
        "# cache usage\n",
        "if 'embedding' not in logs_df.columns:\n",
        "    df_with_emb = compute_embeddings_with_cache(logs_df, text_col='value', model_name='intfloat/e5-small-v2', batch_size=2048)\n",
        "    logs_df = df_with_emb\n",
        "\n",
        "keep_cols = ['datetime', 'embedding'] + ([service_col_name] if service_col_name else [])\n",
        "df_logs = logs_df[keep_cols].copy().reset_index(drop=True)\n",
        "df_logs.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c015be7",
      "metadata": {},
      "outputs": [],
      "source": [
        "LOGS_CLUSTERED_PATH = '/Users/arinagoncharova/Documents/diploma/repo/data/AIOps/preprocessed_while_modelling/e5_small_embeddings/embedded_logs_2022-05-03.parquet'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51a8349",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_logs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_logs\u001b[49m.to_parquet(\u001b[33m'\u001b[39m\u001b[33membedded_logs_2022-05-03.parquet\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'df_logs' is not defined"
          ]
        }
      ],
      "source": [
        "df_logs.to_parquet(LOGS_CLUSTERED_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5fdfdee7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_logs = pd.read_parquet(LOGS_CLUSTERED_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8f58fcf6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:119: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:122: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hdbscan fitting fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hdbscan predicting fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:103: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add_roll)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:103: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add_roll)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training classifier fold 1\n"
          ]
        }
      ],
      "source": [
        "# fast mode for checking\n",
        "SPEED_MODE = True \n",
        "\n",
        "if SPEED_MODE:\n",
        "    # shorten logs -> take 10% of logs\n",
        "    df_logs = df_logs.sample(frac=0.1, random_state=42).sort_values(\"datetime\")\n",
        "\n",
        "    # time window simplification\n",
        "    wcfg = WindowConfig(freq=\"5min\", history_len=3, pre_failure_lead=pd.Timedelta(\"10min\"))\n",
        "\n",
        "    # lighten clustering\n",
        "    ccfg = ClusterConfig(\n",
        "        min_cluster_size=300,\n",
        "        min_samples=150,\n",
        "        cluster_selection_method=\"eom\",\n",
        "        cluster_selection_epsilon=0.03,\n",
        "        # only 10k points are trained\n",
        "        fit_max_points=10000,         \n",
        "    )\n",
        "\n",
        "    mcfg = ModelConfig(random_state=42, class_weight=\"balanced\", max_iter=1000)\n",
        "\n",
        "    # shorter windows\n",
        "    n_splits = 1\n",
        "    min_train_period = \"6H\"\n",
        "    test_period = \"6H\"\n",
        "\n",
        "else:\n",
        "    # original parameters\n",
        "    wcfg = WindowConfig(freq=\"1min\", history_len=5, pre_failure_lead=pd.Timedelta(\"30min\"))\n",
        "    ccfg = ClusterConfig(min_cluster_size=800, min_samples=400, cluster_selection_method=\"eom\", cluster_selection_epsilon=0.03)\n",
        "    mcfg = ModelConfig(random_state=42, class_weight=\"balanced\", max_iter=1000)\n",
        "    n_splits = 1\n",
        "    min_train_period = \"12H\"\n",
        "    test_period = \"12H\"\n",
        "\n",
        "res = train_eval_timewindow(\n",
        "    df_logs=df_logs,\n",
        "    incidents=incidents,\n",
        "    window_cfg=wcfg,\n",
        "    cluster_cfg=ccfg,\n",
        "    model_cfg=mcfg,\n",
        "    service_col=service_col_name,\n",
        "    n_splits=n_splits,\n",
        "    min_train_period=min_train_period,\n",
        "    test_period=test_period,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b467c83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen threshold: 0.4543204362564163\n",
            "Fold 1\n",
            "Train (Timestamp('2022-05-02 16:00:00'), Timestamp('2022-05-02 22:00:00'))\n",
            "Test (Timestamp('2022-05-02 22:00:00'), Timestamp('2022-05-03 04:00:00'))\n",
            "ROC-AUC = 0.502\n",
            "PR-AUC = 0.164 F1@thr = 0.259\n",
            "Detection rate% = 28.6%\n",
            "MTTD = 8.1m \n",
            "FA/day=3581.75\n"
          ]
        }
      ],
      "source": [
        "print(\"Chosen threshold:\", res['chosen_threshold'])\n",
        "for r in res['results']:\n",
        "    print(\n",
        "        f\"Fold {r['fold']}\\n\"\n",
        "        f\"Train {r['train_range']}\\n\"\n",
        "        f\"Test {r['test_range']}\\n\"\n",
        "        f\"ROC-AUC = {r['roc_auc']:.3f}\\n\"\n",
        "        f\"PR-AUC = {r['pr_auc']:.3f} F1@thr = {r['f1_at_thr']:.3f}\\n\"\n",
        "        f\"Detection rate% = {100*r['event_detection_rate']:.1f}%\\n\"\n",
        "        f\"MTTD = {r['mean_lead_minutes']:.1f}m \\n\"\n",
        "        f\"FA/day={r['false_alarms_per_day']:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "732f963f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1:\n",
            "Train: 2022-05-02 16:00:00 - 2022-05-03 04:00:00 has incidents = 10\n",
            "Test: 2022-05-03 04:00:00 - 2022-05-03 15:59:59 has incidents = 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:119: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:122: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n"
          ]
        }
      ],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "# calculate time splits the same way as train_eval_timewindow\n",
        "splits = build_time_splits(\n",
        "    df_logs[\"datetime\"],\n",
        "    n_splits=1,              \n",
        "    min_train_period=\"12H\",\n",
        "    test_period=\"12H\"\n",
        ")\n",
        "\n",
        "# find number of incidents in each train/test range\n",
        "def count_incidents_in_range(inc_df, start, end):\n",
        "    times = pd.to_datetime(inc_df[\"datetime\"])\n",
        "    mask = (times >= start) & (times < end)\n",
        "    return mask.sum()\n",
        "\n",
        "for i, sp in enumerate(splits, 1):\n",
        "    n_train = count_incidents_in_range(incidents, sp.train_start, sp.train_end)\n",
        "    n_test  = count_incidents_in_range(incidents, sp.test_start,  sp.test_end)\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"Train: {sp.train_start} - {sp.train_end} has incidents = {n_train}\")\n",
        "    print(f\"Test: {sp.test_start} - {sp.test_end} has incidents = {n_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5feccf6a",
      "metadata": {},
      "source": [
        "## Experimenting with models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0f420ae2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def build_single_split_features(\n",
        "    df_logs, incidents, window_cfg, cluster_cfg,\n",
        "    service_col=None, time_col=\"datetime\", embedding_col=\"embedding\",\n",
        "    min_train_period=\"12H\", test_period=\"12H\"\n",
        "):\n",
        "    splits = build_time_splits(df_logs[time_col], n_splits=1,\n",
        "                               min_train_period=min_train_period, test_period=test_period)\n",
        "    assert splits, \"Не получилось построить сплит — мало данных\"\n",
        "    sp = splits[0]\n",
        "\n",
        "    df = df_logs.sort_values(time_col).copy()\n",
        "    train_df = df[(df[time_col] >= sp.train_start) & (df[time_col] < sp.train_end)].copy()\n",
        "    test_df  = df[(df[time_col] >= sp.test_start)  & (df[time_col] < sp.test_end)].copy()\n",
        "\n",
        "    Xtr = np.vstack(train_df[embedding_col].to_list())\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=cluster_cfg.metric,\n",
        "        min_cluster_size=cluster_cfg.min_cluster_size,\n",
        "        min_samples=cluster_cfg.min_samples,\n",
        "        cluster_selection_method=cluster_cfg.cluster_selection_method,\n",
        "        cluster_selection_epsilon=cluster_cfg.cluster_selection_epsilon,\n",
        "        prediction_data=True,\n",
        "        approx_min_span_tree=True,\n",
        "    )\n",
        "    if cluster_cfg.fit_max_points and Xtr.shape[0] > cluster_cfg.fit_max_points:\n",
        "        idx = np.random.RandomState(0).choice(Xtr.shape[0], cluster_cfg.fit_max_points, replace=False)\n",
        "        clusterer.fit(Xtr[idx])\n",
        "    else:\n",
        "        clusterer.fit(Xtr)\n",
        "\n",
        "    from hdbscan import approximate_predict\n",
        "    tr_labels, tr_strength = approximate_predict(clusterer, Xtr)\n",
        "    train_df[\"cluster\"] = tr_labels\n",
        "\n",
        "    Xte = np.vstack(test_df[embedding_col].to_list())\n",
        "    te_labels, te_strength = approximate_predict(clusterer, Xte)\n",
        "    test_df[\"cluster\"] = te_labels\n",
        "\n",
        "    pivot_train = resample_and_pivot_counts(train_df, time_col=time_col, cluster_col=\"cluster\",\n",
        "                                            freq=window_cfg.freq, service_col=service_col)\n",
        "    pivot_test  = resample_and_pivot_counts(test_df,  time_col=time_col, cluster_col=\"cluster\",\n",
        "                                            freq=window_cfg.freq, service_col=service_col)\n",
        "\n",
        "    feat_train = add_history_features(pivot_train, time_col=time_col, service_col=service_col,\n",
        "                                      history_len=window_cfg.history_len)\n",
        "    feat_test  = add_history_features(pivot_test,  time_col=time_col, service_col=service_col,\n",
        "                                      history_len=window_cfg.history_len)\n",
        "\n",
        "    y_train = make_labels_by_time(\n",
        "        feat_train[time_col], incidents, lead=window_cfg.pre_failure_lead,\n",
        "        service=feat_train[service_col] if service_col and service_col in feat_train.columns else None\n",
        "    )\n",
        "    y_test = make_labels_by_time(\n",
        "        feat_test[time_col], incidents, lead=window_cfg.pre_failure_lead,\n",
        "        service=feat_test[service_col] if service_col and service_col in feat_test.columns else None\n",
        "    )\n",
        "\n",
        "    drop_cols = [time_col] + ([service_col] if service_col and service_col in feat_train.columns else [])\n",
        "    feature_cols = [c for c in feat_train.columns if c not in drop_cols]\n",
        "\n",
        "    return (feat_train, y_train, feat_test, y_test, feature_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6185e2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# LightGBM is optional (to not restart notebook with missing package)\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    _HAS_LGBM = True\n",
        "except Exception:\n",
        "    _HAS_LGBM = False\n",
        "\n",
        "def make_models_dict():\n",
        "    models = {\n",
        "        \"LogisticRegression\": Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=False)),\n",
        "            (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42))\n",
        "        ]),\n",
        "        \"DecisionTree\": DecisionTreeClassifier(\n",
        "            max_depth=5, class_weight=\"balanced\", random_state=42\n",
        "        ),\n",
        "        \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=200, random_state=42, n_jobs=-1, class_weight=\"balanced_subsample\"\n",
        "        ),\n",
        "        \"NaiveBayes\": Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=False)),\n",
        "            (\"to_dense\", FunctionTransformer(lambda X: X.toarray() if hasattr(X, \"toarray\") else X)),\n",
        "            (\"clf\", GaussianNB())\n",
        "        ]),\n",
        "        \"KNN\": Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=False)),\n",
        "            (\"clf\", KNeighborsClassifier(n_neighbors=5))\n",
        "        ]),\n",
        "    }\n",
        "    if _HAS_LGBM:\n",
        "        models[\"LightGBM\"] = LGBMClassifier(\n",
        "            n_estimators=300, learning_rate=0.05, random_state=42\n",
        "        )\n",
        "    return models\n",
        "\n",
        "def _predict_proba_or_score(model, Xtr, Xte):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba_tr = model.predict_proba(Xtr)[:, 1]\n",
        "        proba_te = model.predict_proba(Xte)[:, 1]\n",
        "    else:\n",
        "        dec_tr = model.decision_function(Xtr)\n",
        "        dec_te = model.decision_function(Xte)\n",
        "        mm = MinMaxScaler()\n",
        "        proba_tr = mm.fit_transform(dec_tr.reshape(-1,1)).ravel()\n",
        "        proba_te = mm.transform(dec_te.reshape(-1,1)).ravel()\n",
        "    return proba_tr, proba_te\n",
        "\n",
        "def _pick_thr_by_f1(y_true, y_score):\n",
        "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
        "    if len(thr) == 0:\n",
        "        return 0.5\n",
        "    f1s = (2*prec[:-1]*rec[:-1]) / np.clip(prec[:-1]+rec[:-1], 1e-9, None)\n",
        "    return float(thr[int(np.nanargmax(f1s))])\n",
        "\n",
        "def compare_custom_models(Xtr, ytr, Xte, yte, feature_names, pick_by=\"pr_auc\"):\n",
        "    models = make_models_dict()\n",
        "    rows, per_model = [], {}\n",
        "\n",
        "    for name, mdl in models.items():\n",
        "        mdl.fit(Xtr, ytr)\n",
        "        proba_tr, proba_te = _predict_proba_or_score(mdl, Xtr, Xte)\n",
        "        thr = _pick_thr_by_f1(ytr, proba_tr)\n",
        "        pred_te = (proba_te >= thr).astype(int)\n",
        "\n",
        "        roc = roc_auc_score(yte, proba_te) if len(np.unique(yte))>1 else np.nan\n",
        "        pra = average_precision_score(yte, proba_te) if len(np.unique(yte))>1 else np.nan\n",
        "        f1  = f1_score(yte, pred_te) if len(np.unique(yte))>1 else np.nan\n",
        "        rep = classification_report(yte, pred_te, digits=3, zero_division=0)\n",
        "\n",
        "        # importances\n",
        "        importances, kind = None, None\n",
        "        try:\n",
        "            base = mdl.named_steps[\"clf\"] if hasattr(mdl, \"named_steps\") and \"clf\" in mdl.named_steps else mdl\n",
        "            if hasattr(base, \"feature_importances_\"):\n",
        "                importances = base.feature_importances_\n",
        "                kind = \"model_feature_importances_\"\n",
        "            elif hasattr(base, \"coef_\"):\n",
        "                importances = np.abs(base.coef_).ravel()\n",
        "                kind = \"|coef_|\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        fi_df = None\n",
        "        if importances is not None and len(importances) == len(feature_names):\n",
        "            fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\n",
        "                \"importance\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "        rows.append({\"model\": name, \"roc_auc\": roc, \"pr_auc\": pra, \"f1_at_thr\": f1, \"thr\": thr})\n",
        "        per_model[name] = {\n",
        "            \"classifier\": mdl, \"proba_test\": proba_te, \"pred_test\": pred_te,\n",
        "            \"thr\": thr, \"classification_report\": rep,\n",
        "            \"feature_importance\": fi_df, \"importance_kind\": kind\n",
        "        }\n",
        "\n",
        "    rep_df = pd.DataFrame(rows).sort_values(pick_by, ascending=False)\n",
        "    best_name = rep_df.iloc[0][\"model\"]\n",
        "    return rep_df, best_name, per_model[best_name], per_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "820ee74d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:119: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:122: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:103: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add_roll)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3202242798.py:103: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add_roll)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 446, number of negative: 2773\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003116 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2367\n",
            "[LightGBM] [Info] Number of data points in the train set: 3219, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.138552 -> initscore=-1.827366\n",
            "[LightGBM] [Info] Start training from score -1.827366\n",
            "Best simple model is  DecisionTree  with a threshold: 0.5029221180195244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "roc_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pr_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "f1_at_thr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "thr",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "b1dd5301-ad26-4c57-bfc6-2466121ad326",
              "rows": [
                [
                  "1",
                  "DecisionTree",
                  "0.5090371609279538",
                  "0.15240335213080566",
                  "0.2587183308494784",
                  "0.5029221180195244"
                ],
                [
                  "4",
                  "KNN",
                  "0.5028236353757475",
                  "0.15066335812789214",
                  "0.14699792960662525",
                  "0.4"
                ],
                [
                  "2",
                  "RandomForest",
                  "0.5069049874261543",
                  "0.14934739881771555",
                  "0.07407407407407407",
                  "0.5550258660759584"
                ],
                [
                  "5",
                  "LightGBM",
                  "0.4995486567306939",
                  "0.1482764375543913",
                  "0.0469208211143695",
                  "0.3216995926867263"
                ],
                [
                  "0",
                  "LogisticRegression",
                  "0.5043231312263119",
                  "0.14590727374245252",
                  "0.24713083397092578",
                  "0.4761457685468217"
                ],
                [
                  "3",
                  "NaiveBayes",
                  "0.500701757415779",
                  "0.14574508420296936",
                  "0.2602864980188967",
                  "4.230971381435618e-05"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>f1_at_thr</th>\n",
              "      <th>thr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.509037</td>\n",
              "      <td>0.152403</td>\n",
              "      <td>0.258718</td>\n",
              "      <td>0.502922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.502824</td>\n",
              "      <td>0.150663</td>\n",
              "      <td>0.146998</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.506905</td>\n",
              "      <td>0.149347</td>\n",
              "      <td>0.074074</td>\n",
              "      <td>0.555026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.499549</td>\n",
              "      <td>0.148276</td>\n",
              "      <td>0.046921</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.504323</td>\n",
              "      <td>0.145907</td>\n",
              "      <td>0.247131</td>\n",
              "      <td>0.476146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaiveBayes</td>\n",
              "      <td>0.500702</td>\n",
              "      <td>0.145745</td>\n",
              "      <td>0.260286</td>\n",
              "      <td>0.000042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model   roc_auc    pr_auc  f1_at_thr       thr\n",
              "1        DecisionTree  0.509037  0.152403   0.258718  0.502922\n",
              "4                 KNN  0.502824  0.150663   0.146998  0.400000\n",
              "2        RandomForest  0.506905  0.149347   0.074074  0.555026\n",
              "5            LightGBM  0.499549  0.148276   0.046921  0.321700\n",
              "0  LogisticRegression  0.504323  0.145907   0.247131  0.476146\n",
              "3          NaiveBayes  0.500702  0.145745   0.260286  0.000042"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_feat = build_single_split_features(\n",
        "    df_logs=df_logs, incidents=incidents, window_cfg=wcfg, cluster_cfg=ccfg,\n",
        "    service_col=service_col_name, min_train_period=\"12H\", test_period=\"12H\"\n",
        ")\n",
        "feat_train, y_train, feat_test, y_test, feature_cols = _feat\n",
        "\n",
        "Xtr = feat_train[feature_cols].values\n",
        "Xte = feat_test[feature_cols].values\n",
        "\n",
        "rep_df2, best_name2, best_out2, all_out2 = compare_custom_models(\n",
        "    Xtr, y_train.values, Xte, y_test.values, feature_cols, pick_by=\"pr_auc\"\n",
        ")\n",
        "print(\"Best simple model is \", best_name2, \" with a threshold:\", best_out2[\"thr\"])\n",
        "rep_df2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f6894071",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alerts total: 2841\n",
            "Detected incidents: 11 of 11\n",
            "False alarms: 2407\n",
            "Source of feature importances: model_feature_importances_\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "feature",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "importance",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "8af0662b-97f3-46d2-8c4f-85878c6ff289",
              "rows": [
                [
                  "0",
                  "4_mean3",
                  "0.19556002147119866"
                ],
                [
                  "1",
                  "0_mean3",
                  "0.19414503288087656"
                ],
                [
                  "2",
                  "13_mean3",
                  "0.18426106191701636"
                ],
                [
                  "3",
                  "14_mean3",
                  "0.1704633160607575"
                ],
                [
                  "4",
                  "6_mean3",
                  "0.16926436989004617"
                ],
                [
                  "5",
                  "6_diff1",
                  "0.037239876960195345"
                ],
                [
                  "6",
                  "8_mean3",
                  "0.02393920809521011"
                ],
                [
                  "7",
                  "8",
                  "0.012717502901654137"
                ],
                [
                  "8",
                  "13_diff1",
                  "0.01240960982304525"
                ],
                [
                  "9",
                  "4_diff1",
                  "0.0"
                ],
                [
                  "10",
                  "5_mean3",
                  "0.0"
                ],
                [
                  "11",
                  "5_diff1",
                  "0.0"
                ],
                [
                  "12",
                  "7_mean3",
                  "0.0"
                ],
                [
                  "13",
                  "7_diff1",
                  "0.0"
                ],
                [
                  "14",
                  "-1",
                  "0.0"
                ],
                [
                  "15",
                  "8_diff1",
                  "0.0"
                ],
                [
                  "16",
                  "9_mean3",
                  "0.0"
                ],
                [
                  "17",
                  "3_diff1",
                  "0.0"
                ],
                [
                  "18",
                  "10_mean3",
                  "0.0"
                ],
                [
                  "19",
                  "10_diff1",
                  "0.0"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 20
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4_mean3</td>\n",
              "      <td>0.195560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_mean3</td>\n",
              "      <td>0.194145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13_mean3</td>\n",
              "      <td>0.184261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14_mean3</td>\n",
              "      <td>0.170463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6_mean3</td>\n",
              "      <td>0.169264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6_diff1</td>\n",
              "      <td>0.037240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8_mean3</td>\n",
              "      <td>0.023939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.012718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13_diff1</td>\n",
              "      <td>0.012410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5_mean3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7_mean3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>9_mean3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10_mean3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10_diff1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature  importance\n",
              "0    4_mean3    0.195560\n",
              "1    0_mean3    0.194145\n",
              "2   13_mean3    0.184261\n",
              "3   14_mean3    0.170463\n",
              "4    6_mean3    0.169264\n",
              "5    6_diff1    0.037240\n",
              "6    8_mean3    0.023939\n",
              "7          8    0.012718\n",
              "8   13_diff1    0.012410\n",
              "9    4_diff1    0.000000\n",
              "10   5_mean3    0.000000\n",
              "11   5_diff1    0.000000\n",
              "12   7_mean3    0.000000\n",
              "13   7_diff1    0.000000\n",
              "14        -1    0.000000\n",
              "15   8_diff1    0.000000\n",
              "16   9_mean3    0.000000\n",
              "17   3_diff1    0.000000\n",
              "18  10_mean3    0.000000\n",
              "19  10_diff1    0.000000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Detailed analysis of the best model's predictions over time\n",
        "timeline = feat_test[[\"datetime\"]].copy()\n",
        "timeline[\"y_true\"] = y_test.values.astype(int)\n",
        "timeline[\"proba\"]  = best_out2[\"proba_test\"]\n",
        "timeline[\"pred\"]   = best_out2[\"pred_test\"]\n",
        "\n",
        "fires = timeline[timeline[\"pred\"] == 1].copy()\n",
        "\n",
        "lead = wcfg.pre_failure_lead\n",
        "inc_test = incidents[(incidents[\"datetime\"] >= timeline[\"datetime\"].min()) &\n",
        "                     (incidents[\"datetime\"] <= timeline[\"datetime\"].max())].copy()\n",
        "\n",
        "inc_test[\"first_fire\"] = pd.NaT\n",
        "inc_test[\"lead_minutes\"] = np.nan\n",
        "for i, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    mask = (timeline[\"datetime\"] > t0 - lead) & (timeline[\"datetime\"] <= t0) & (timeline[\"pred\"] == 1)\n",
        "    if mask.any():\n",
        "        t_first = timeline.loc[mask, \"datetime\"].min()\n",
        "        inc_test.at[i, \"first_fire\"] = t_first\n",
        "        inc_test.at[i, \"lead_minutes\"] = (t0 - t_first).total_seconds()/60.0\n",
        "\n",
        "detected = inc_test[inc_test[\"first_fire\"].notna()].copy()\n",
        "missed   = inc_test[inc_test[\"first_fire\"].isna()].copy()\n",
        "\n",
        "true_alert_mask = pd.Series(False, index=fires.index)\n",
        "for _, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    win = (fires[\"datetime\"] > t0 - lead) & (fires[\"datetime\"] <= t0)\n",
        "    true_alert_mask |= win\n",
        "false_alarms = fires[~true_alert_mask].copy()\n",
        "\n",
        "print(\"Alerts total:\", len(fires))\n",
        "print(\"Detected incidents:\", len(detected), \"of\", len(inc_test))\n",
        "print(\"False alarms:\", len(false_alarms))\n",
        "\n",
        "fi = best_out2[\"feature_importance\"]\n",
        "if fi is not None:\n",
        "    print(\"Source of feature importances:\", best_out2[\"importance_kind\"])\n",
        "    display(fi.head(20))\n",
        "\n",
        "timeline.to_csv(\"results/pred_timeline_best.csv\", index=False)\n",
        "detected[[\"datetime\",\"first_fire\",\"lead_minutes\"]].to_csv(\"results/incidents_detected.csv\", index=False)\n",
        "missed[[\"datetime\"]].to_csv(\"results/incidents_missed.csv\", index=False)\n",
        "false_alarms[[\"datetime\"]].to_csv(\"results/false_alarms.csv\", index=False)\n",
        "if fi is not None:\n",
        "    fi.to_csv(\"results/feature_importance_best.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:80: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:83: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 298, number of negative: 422\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2596\n",
            "[LightGBM] [Info] Number of data points in the train set: 720, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.413889 -> initscore=-0.347912\n",
            "[LightGBM] [Info] Start training from score -0.347912\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best model (logs): RandomForest | threshold: 0.645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "roc_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pr_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "f1_at_thr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "thr",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "160c3cff-66a5-48b9-b379-0a4c112007d1",
              "rows": [
                [
                  "2",
                  "RandomForest",
                  "0.6918888426577904",
                  "0.6386646775655853",
                  "0.43309002433090027",
                  "0.645"
                ],
                [
                  "5",
                  "LightGBM",
                  "0.6586064524388682",
                  "0.6034424173169994",
                  "0.35658914728682173",
                  "0.9868099953850173"
                ],
                [
                  "4",
                  "KNN",
                  "0.6312652029189604",
                  "0.5507970244123461",
                  "0.5401709401709401",
                  "0.4"
                ],
                [
                  "3",
                  "NaiveBayes",
                  "0.5431282806298809",
                  "0.5202450473765245",
                  "0.45774647887323944",
                  "1.4044841898475695e-13"
                ],
                [
                  "1",
                  "DecisionTree",
                  "0.6022316284726668",
                  "0.491915997750892",
                  "0.5254901960784314",
                  "0.5861111111111111"
                ],
                [
                  "0",
                  "LogisticRegression",
                  "0.5202918960440405",
                  "0.4167209132472063",
                  "0.48120300751879697",
                  "0.39225764903926075"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>f1_at_thr</th>\n",
              "      <th>thr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.691889</td>\n",
              "      <td>0.638665</td>\n",
              "      <td>0.433090</td>\n",
              "      <td>6.450000e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.658606</td>\n",
              "      <td>0.603442</td>\n",
              "      <td>0.356589</td>\n",
              "      <td>9.868100e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.631265</td>\n",
              "      <td>0.550797</td>\n",
              "      <td>0.540171</td>\n",
              "      <td>4.000000e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaiveBayes</td>\n",
              "      <td>0.543128</td>\n",
              "      <td>0.520245</td>\n",
              "      <td>0.457746</td>\n",
              "      <td>1.404484e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.602232</td>\n",
              "      <td>0.491916</td>\n",
              "      <td>0.525490</td>\n",
              "      <td>5.861111e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.520292</td>\n",
              "      <td>0.416721</td>\n",
              "      <td>0.481203</td>\n",
              "      <td>3.922576e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model   roc_auc    pr_auc  f1_at_thr           thr\n",
              "2        RandomForest  0.691889  0.638665   0.433090  6.450000e-01\n",
              "5            LightGBM  0.658606  0.603442   0.356589  9.868100e-01\n",
              "4                 KNN  0.631265  0.550797   0.540171  4.000000e-01\n",
              "3          NaiveBayes  0.543128  0.520245   0.457746  1.404484e-13\n",
              "1        DecisionTree  0.602232  0.491916   0.525490  5.861111e-01\n",
              "0  LogisticRegression  0.520292  0.416721   0.481203  3.922576e-01"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected incidents: 6 / 11\n",
            "False alarms: 30\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "# get features\n",
        "feat_train, y_train, feat_test, y_test, feature_cols = build_single_split_features(\n",
        "    df_logs=df_logs,\n",
        "    incidents=incidents,                \n",
        "    window_cfg=wcfg,\n",
        "    cluster_cfg=ccfg,\n",
        "    service_col=service_col_name,\n",
        "    min_train_period=\"12H\",\n",
        "    test_period=\"12H\",\n",
        ")\n",
        "\n",
        "Xtr = feat_train[feature_cols].values\n",
        "Xte = feat_test[feature_cols].values\n",
        "\n",
        "# compare and select the best model\n",
        "rep_l, best_l_name, best_l_out, all_l_out = compare_custom_models(\n",
        "    Xtr, y_train.values, Xte, y_test.values, feature_cols, pick_by=\"pr_auc\"\n",
        ")\n",
        "print(\"Best model (logs):\", best_l_name, \"| threshold:\", best_l_out[\"thr\"])\n",
        "display(rep_l)\n",
        "\n",
        "# prediction artifacts\n",
        "timeline = feat_test[[\"datetime\"]].copy()\n",
        "timeline[\"y_true\"] = y_test.values.astype(int)\n",
        "timeline[\"proba\"]  = best_l_out[\"proba_test\"]\n",
        "timeline[\"pred\"]   = best_l_out[\"pred_test\"]\n",
        "\n",
        "lead = wcfg.pre_failure_lead\n",
        "inc = incidents.copy()\n",
        "inc[\"datetime\"] = pd.to_datetime(inc[\"datetime\"])\n",
        "\n",
        "inc_test = inc[(inc[\"datetime\"] >= timeline[\"datetime\"].min()) &\n",
        "               (inc[\"datetime\"] <= timeline[\"datetime\"].max())].copy()\n",
        "inc_test[\"first_fire\"] = pd.NaT\n",
        "inc_test[\"lead_minutes\"] = np.nan\n",
        "\n",
        "for i, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    mask = (timeline[\"datetime\"] > t0 - lead) & (timeline[\"datetime\"] <= t0) & (timeline[\"pred\"] == 1)\n",
        "    if mask.any():\n",
        "        t_first = timeline.loc[mask, \"datetime\"].min()\n",
        "        inc_test.at[i, \"first_fire\"]   = t_first\n",
        "        inc_test.at[i, \"lead_minutes\"] = (t0 - t_first).total_seconds()/60.0\n",
        "\n",
        "detected = inc_test[inc_test[\"first_fire\"].notna()].copy()\n",
        "missed   = inc_test[inc_test[\"first_fire\"].isna()].copy()\n",
        "\n",
        "fires = timeline[timeline[\"pred\"] == 1].copy()\n",
        "true_alert_mask = pd.Series(False, index=fires.index)\n",
        "for _, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    true_alert_mask |= ((fires[\"datetime\"] > t0 - lead) & (fires[\"datetime\"] <= t0))\n",
        "false_alarms = fires[~true_alert_mask].copy()\n",
        "\n",
        "print(f\"Detected incidents: {len(detected)} / {len(inc_test)}\")\n",
        "print(f\"False alarms: {len(false_alarms)}\")\n",
        "\n",
        "\n",
        "# with open(outdir / \"logs_eval_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump({\n",
        "#         \"best_model\": best_l_name,\n",
        "#         \"threshold\": float(best_l_out[\"thr\"]),\n",
        "#         \"train_range\": [str(feat_train[\"datetime\"].min()), str(feat_train[\"datetime\"].max())],\n",
        "#         \"test_range\":  [str(feat_test[\"datetime\"].min()),  str(feat_test[\"datetime\"].max())],\n",
        "#         \"lead_minutes\": int(lead.total_seconds() // 60),\n",
        "#         \"n_features\": len(feature_cols)\n",
        "#     }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a03256",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save results\n",
        "outdir = Path(\"results/only_logs_simplified\")\n",
        "best_clf = all_l_out[best_l_name][\"classifier\"]\n",
        "joblib.dump(best_clf, outdir / \"best_logs_model.joblib\")\n",
        "\n",
        "timeline.to_csv(outdir / \"logs_pred_timeline.csv\", index=False)\n",
        "detected[[\"datetime\",\"first_fire\",\"lead_minutes\"]].to_csv(outdir / \"logs_incidents_detected.csv\", index=False)\n",
        "missed[[\"datetime\"]].to_csv(outdir / \"logs_incidents_missed.csv\", index=False)\n",
        "false_alarms[[\"datetime\"]].to_csv(outdir / \"logs_false_alarms.csv\", index=False)\n",
        "rep_l.to_csv(outdir / \"logs_models_report.csv\", index=False)\n",
        "\n",
        "fi_df = all_l_out[best_l_name][\"feature_importance\"]\n",
        "if fi_df is not None:\n",
        "    fi_df.to_csv(outdir / \"logs_feature_importance.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7dd5f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs interval: 2022-05-02 16:00:00 -  2022-05-03 15:59:58 | Duration: 0 days 23:59:58\n",
            "Unique minutes: 1434\n",
            "Total incidents: 21 | min: 2022-05-02 18:02:09 | max: 2022-05-03 14:53:16\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_logs[\"datetime\"] = pd.to_datetime(df_logs[\"datetime\"])\n",
        "tmin = df_logs[\"datetime\"].min()\n",
        "tmax = df_logs[\"datetime\"].max()\n",
        "span = tmax - tmin\n",
        "\n",
        "print(\"Logs interval:\", tmin, \"- \", tmax, \"| Duration:\", span)\n",
        "\n",
        "uniq_minutes = df_logs[\"datetime\"].dt.floor(\"min\").nunique()\n",
        "print(\"Unique minutes:\", uniq_minutes)\n",
        "\n",
        "# incidents info\n",
        "if not incidents.empty:\n",
        "    print(\"Total incidents:\", len(incidents), \n",
        "          \"| min:\", pd.to_datetime(incidents[\"datetime\"]).min(),\n",
        "          \"| max:\", pd.to_datetime(incidents[\"datetime\"]).max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55a2e90",
      "metadata": {},
      "source": [
        "## Only metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5a3b02b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total metrics: 7,599,784\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "timestamp",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "cmdb_id",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "kpi_name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "value",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "service",
                  "rawType": "object",
                  "type": "unknown"
                },
                {
                  "name": "rr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "sr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "mrt",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "count",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "0ed0affa-2f03-4e3c-8bff-dcac6923e750",
              "rows": [
                [
                  "0",
                  "1651507200",
                  "istio-egressgateway-7bfdcc9d86-g2d4q",
                  "istio_agent_startup_duration_seconds",
                  "1.3467642130000002",
                  null,
                  null,
                  null,
                  null,
                  null
                ],
                [
                  "1",
                  "1651507200",
                  "istio-ingressgateway-565bffd4d-rn678",
                  "istio_agent_startup_duration_seconds",
                  "1.5292173830000002",
                  null,
                  null,
                  null,
                  null,
                  null
                ],
                [
                  "2",
                  "1651507260",
                  "istio-egressgateway-7bfdcc9d86-g2d4q",
                  "istio_agent_startup_duration_seconds",
                  "1.3467642130000002",
                  null,
                  null,
                  null,
                  null,
                  null
                ],
                [
                  "3",
                  "1651507260",
                  "istio-ingressgateway-565bffd4d-rn678",
                  "istio_agent_startup_duration_seconds",
                  "1.5292173830000002",
                  null,
                  null,
                  null,
                  null,
                  null
                ],
                [
                  "4",
                  "1651507320",
                  "istio-egressgateway-7bfdcc9d86-g2d4q",
                  "istio_agent_startup_duration_seconds",
                  "1.3467642130000002",
                  null,
                  null,
                  null,
                  null,
                  null
                ]
              ],
              "shape": {
                "columns": 9,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>cmdb_id</th>\n",
              "      <th>kpi_name</th>\n",
              "      <th>value</th>\n",
              "      <th>service</th>\n",
              "      <th>rr</th>\n",
              "      <th>sr</th>\n",
              "      <th>mrt</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1651507200</td>\n",
              "      <td>istio-egressgateway-7bfdcc9d86-g2d4q</td>\n",
              "      <td>istio_agent_startup_duration_seconds</td>\n",
              "      <td>1.346764</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1651507200</td>\n",
              "      <td>istio-ingressgateway-565bffd4d-rn678</td>\n",
              "      <td>istio_agent_startup_duration_seconds</td>\n",
              "      <td>1.529217</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1651507260</td>\n",
              "      <td>istio-egressgateway-7bfdcc9d86-g2d4q</td>\n",
              "      <td>istio_agent_startup_duration_seconds</td>\n",
              "      <td>1.346764</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1651507260</td>\n",
              "      <td>istio-ingressgateway-565bffd4d-rn678</td>\n",
              "      <td>istio_agent_startup_duration_seconds</td>\n",
              "      <td>1.529217</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1651507320</td>\n",
              "      <td>istio-egressgateway-7bfdcc9d86-g2d4q</td>\n",
              "      <td>istio_agent_startup_duration_seconds</td>\n",
              "      <td>1.346764</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    timestamp                               cmdb_id  \\\n",
              "0  1651507200  istio-egressgateway-7bfdcc9d86-g2d4q   \n",
              "1  1651507200  istio-ingressgateway-565bffd4d-rn678   \n",
              "2  1651507260  istio-egressgateway-7bfdcc9d86-g2d4q   \n",
              "3  1651507260  istio-ingressgateway-565bffd4d-rn678   \n",
              "4  1651507320  istio-egressgateway-7bfdcc9d86-g2d4q   \n",
              "\n",
              "                               kpi_name     value service  rr  sr  mrt  count  \n",
              "0  istio_agent_startup_duration_seconds  1.346764     NaN NaN NaN  NaN    NaN  \n",
              "1  istio_agent_startup_duration_seconds  1.529217     NaN NaN NaN  NaN    NaN  \n",
              "2  istio_agent_startup_duration_seconds  1.346764     NaN NaN NaN  NaN    NaN  \n",
              "3  istio_agent_startup_duration_seconds  1.529217     NaN NaN NaN  NaN    NaN  \n",
              "4  istio_agent_startup_duration_seconds  1.346764     NaN NaN NaN  NaN    NaN  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "metrics_root = \"/Users/arinagoncharova/Documents/diploma/EDA/Aiops-Dataset/data/2022-05-03/metric\"\n",
        "\n",
        "all_metrics = []\n",
        "\n",
        "for root, dirs, files in os.walk(metrics_root):\n",
        "    for f in files:\n",
        "        if f.endswith(\".csv\"):\n",
        "            file_path = os.path.join(root, f)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                all_metrics.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Error while reading {file_path}: {e}\")\n",
        "\n",
        "# combine all metrics into a single dataframe\n",
        "metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
        "print(f\"Total metrics: {len(metrics_df):,}\")\n",
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70563e67",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregated metrics shape: (14392, 6)\n",
            "Services (metrics, normalized) sample: ['adservice' 'cartservice' 'checkoutservice' 'currencyservice'\n",
            " 'emailservice' 'frontend' 'paymentservice' 'productcatalogservice'\n",
            " 'recommendationservice' 'shippingservice']\n",
            "Services (incidents, normalized) sample: ['productcatalogservice' 'recommendationservice' 'frontend' 'cartservice'\n",
            " 'emailservice' 'adservice' 'currencyservice' 'checkoutservice']\n",
            "Train window: 2022-05-02 16:00:00 → 2022-05-03 04:00:00 | positives: 310 / 7200\n",
            "Test  window: 2022-05-03 04:00:00 → 2022-05-03 15:59:00 | positives: 320 / 7182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:80: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:83: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def normalize_service_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    s = str(s).strip().lower()\n",
        "    # strip protocol suffixes and port-ish tails\n",
        "    s = re.sub(r\"-(grpc|http|https)$\", \"\", s)\n",
        "    # optional: strip trailing '-svc' if your data uses that (disabled by default)\n",
        "    # s = re.sub(r\"-svc$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# --- 2) Prepare metrics (already wide with rr/sr/mrt/count)\n",
        "metrics_wide = metrics_use.copy()\n",
        "# ensure datetime\n",
        "metrics_wide[\"datetime\"] = pd.to_datetime(metrics_wide[\"datetime\"])\n",
        "# ensure a service column, then normalize to 'service_base'\n",
        "if \"service\" not in metrics_wide.columns:\n",
        "    if \"cmdb_id\" in metrics_wide.columns:\n",
        "        metrics_wide = metrics_wide.rename(columns={\"cmdb_id\": \"service\"})\n",
        "    else:\n",
        "        metrics_wide[\"service\"] = \"GLOBAL\"\n",
        "metrics_wide[\"service_base\"] = metrics_wide[\"service\"].map(normalize_service_name)\n",
        "\n",
        "# numeric\n",
        "for c in [\"rr\",\"sr\",\"mrt\",\"count\"]:\n",
        "    if c in metrics_wide.columns:\n",
        "        metrics_wide[c] = pd.to_numeric(metrics_wide[c], errors=\"coerce\")\n",
        "\n",
        "# aggregate per minute × normalized service\n",
        "metrics_high_agg = (\n",
        "    metrics_wide\n",
        "    .groupby([pd.Grouper(key=\"datetime\", freq=\"1min\"), \"service_base\"], as_index=False)\n",
        "    .agg({\"rr\": \"mean\", \"sr\": \"mean\", \"mrt\": \"mean\", \"count\": \"sum\"})\n",
        "    .rename(columns={\"service_base\":\"service\"})\n",
        ")\n",
        "\n",
        "print(\"Aggregated metrics shape:\", metrics_high_agg.shape)\n",
        "print(\"Services (metrics, normalized) sample:\", metrics_high_agg[\"service\"].dropna().unique()[:10])\n",
        "\n",
        "# prepare incidents with the same normalized service\n",
        "inc = service_failures_03_05_df.copy()\n",
        "inc[\"datetime\"] = pd.to_datetime(inc[\"datetime\"])\n",
        "\n",
        "# find which col stores service on incidents and normalize into 'service'\n",
        "if \"service\" in inc.columns:\n",
        "    inc[\"service\"] = inc[\"service\"].map(normalize_service_name)\n",
        "elif \"cmdb_id\" in inc.columns:\n",
        "    inc[\"service\"] = inc[\"cmdb_id\"].map(normalize_service_name)\n",
        "else:\n",
        "    # if no per-service info, keep global (labels will be global)\n",
        "    inc[\"service\"] = None\n",
        "\n",
        "print(\"Services (incidents, normalized) sample:\", inc[\"service\"].dropna().unique()[:10])\n",
        "\n",
        "# history features + labeling on the wide table\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class WindowConfig:\n",
        "    freq: str = \"1min\"\n",
        "    history_len: int = 5\n",
        "    pre_failure_lead: pd.Timedelta = pd.Timedelta(\"30min\")\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    train_start: pd.Timestamp\n",
        "    train_end: pd.Timestamp\n",
        "    test_start: pd.Timestamp\n",
        "    test_end: pd.Timestamp\n",
        "\n",
        "def build_time_splits(times: pd.Series, n_splits=1, min_train_period=\"12H\", test_period=\"12H\") -> List[Split]:\n",
        "    times = pd.to_datetime(times).sort_values()\n",
        "    start, end = times.min(), times.max()\n",
        "    splits = []\n",
        "    train_end = start + pd.Timedelta(min_train_period)\n",
        "    for _ in range(n_splits):\n",
        "        test_start = train_end\n",
        "        test_end = min(test_start + pd.Timedelta(test_period), end)\n",
        "        if test_start >= test_end: break\n",
        "        splits.append(Split(start, train_end, test_start, test_end))\n",
        "        train_end = test_end\n",
        "    return splits\n",
        "\n",
        "def add_history_features(pivot: pd.DataFrame, time_col=\"datetime\", service_col: Optional[str]=None, history_len=5) -> pd.DataFrame:\n",
        "    df = pivot.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    base_cols = [time_col] + ([service_col] if service_col and service_col in df.columns else [])\n",
        "    metric_cols = [c for c in df.columns if c not in base_cols]\n",
        "    def _add(g: pd.DataFrame):\n",
        "        g = g.sort_values(time_col)\n",
        "        for c in metric_cols:\n",
        "            g[f\"{c}_mean{history_len}\"] = g[c].rolling(window=history_len, min_periods=1).mean()\n",
        "            g[f\"{c}_diff1\"] = g[c].diff().fillna(0)\n",
        "        return g\n",
        "    if service_col and service_col in df.columns:\n",
        "        df = df.groupby(service_col, group_keys=False).apply(_add)\n",
        "    else:\n",
        "        df = _add(df)\n",
        "    return df\n",
        "\n",
        "def make_labels_by_time(times: pd.Series, incidents: pd.DataFrame, lead: pd.Timedelta,\n",
        "                        service: Optional[pd.Series]=None) -> pd.Series:\n",
        "    times = pd.to_datetime(times)\n",
        "    y = pd.Series(0, index=times.index, dtype=int)\n",
        "    inc = incidents.copy()\n",
        "    # will use 'service' if present & notna; else global\n",
        "    has_service = \"service\" in inc.columns and inc[\"service\"].notna().any()\n",
        "    if service is not None and has_service:\n",
        "        service = service.astype(str)\n",
        "        for srv, grp in inc.dropna(subset=[\"service\"]).groupby(\"service\"):\n",
        "            mask_srv = (service == str(srv))\n",
        "            if not mask_srv.any(): continue\n",
        "            t_srv = times[mask_srv]\n",
        "            y_srv = pd.Series(0, index=t_srv.index, dtype=int)\n",
        "            for t0 in pd.to_datetime(grp[\"datetime\"]).sort_values():\n",
        "                win = (t_srv > (t0 - lead)) & (t_srv <= t0)\n",
        "                if win.any(): y_srv.loc[win] = 1\n",
        "            y.loc[y_srv.index] = y_srv.values\n",
        "    else:\n",
        "        for t0 in pd.to_datetime(inc[\"datetime\"]).sort_values():\n",
        "            win = (times > (t0 - lead)) & (times <= t0)\n",
        "            if win.any(): y.loc[win.index[win]] = 1\n",
        "    return y\n",
        "\n",
        "def build_features_from_wide(\n",
        "    df: pd.DataFrame,\n",
        "    incidents: pd.DataFrame,\n",
        "    wcfg: WindowConfig,\n",
        "    time_col=\"datetime\",\n",
        "    service_col=\"service\",\n",
        "    min_train_period=\"12H\",\n",
        "    test_period=\"12H\"\n",
        "):\n",
        "    splits = build_time_splits(df[time_col], n_splits=1,\n",
        "                               min_train_period=min_train_period, test_period=test_period)\n",
        "    assert splits, \"Not enough data for requested windows\"\n",
        "    sp = splits[0]\n",
        "    train_df = df[(df[time_col] >= sp.train_start) & (df[time_col] < sp.train_end)].copy()\n",
        "    test_df  = df[(df[time_col] >= sp.test_start)  & (df[time_col] < sp.test_end)].copy()\n",
        "\n",
        "    feat_train = add_history_features(train_df, time_col=time_col, service_col=service_col, history_len=wcfg.history_len)\n",
        "    feat_test  = add_history_features(test_df,  time_col=time_col, service_col=service_col, history_len=wcfg.history_len)\n",
        "\n",
        "    y_train = make_labels_by_time(\n",
        "        feat_train[time_col], incidents, wcfg.pre_failure_lead,\n",
        "        service=feat_train[service_col] if service_col in feat_train.columns else None\n",
        "    )\n",
        "    y_test = make_labels_by_time(\n",
        "        feat_test[time_col], incidents, wcfg.pre_failure_lead,\n",
        "        service=feat_test[service_col] if service_col in feat_test.columns else None\n",
        "    )\n",
        "\n",
        "    drop_cols = [time_col, service_col]\n",
        "    feature_cols = [c for c in feat_train.columns if c not in drop_cols]\n",
        "    Xtr = feat_train[feature_cols].values\n",
        "    Xte = feat_test[feature_cols].values\n",
        "    return feat_train, y_train, feat_test, y_test, feature_cols, Xtr, Xte, sp\n",
        "\n",
        "# --- 6) Build features & labels using NORMALIZED services\n",
        "wcfg_metrics = WindowConfig(freq=\"1min\", history_len=5, pre_failure_lead=pd.Timedelta(\"30min\"))\n",
        "\n",
        "feat_train_m, y_train_m, feat_test_m, y_test_m, feature_cols_m, Xtr_m, Xte_m, sp_m = build_features_from_wide(\n",
        "    df=metrics_high_agg,     # aggregated by minute × normalized service\n",
        "    incidents=inc,           # incidents with normalized 'service'\n",
        "    wcfg=wcfg_metrics,\n",
        "    time_col=\"datetime\",\n",
        "    service_col=\"service\",   # <- important: use normalized service\n",
        "    min_train_period=\"12H\", test_period=\"12H\"\n",
        ")\n",
        "\n",
        "print(\"Train window:\", sp_m.train_start, \"→\", sp_m.train_end,\n",
        "      \"| positives:\", int(y_train_m.sum()), \"/\", len(y_train_m))\n",
        "print(\"Test  window:\", sp_m.test_start,  \"→\", sp_m.test_end,\n",
        "      \"| positives:\", int(y_test_m.sum()),  \"/\", len(y_test_m))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a2364ed7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "\n",
            "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 310, number of negative: 6890\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2261\n",
            "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043056 -> initscore=-3.101254\n",
            "[LightGBM] [Info] Start training from score -3.101254\n",
            "Best model is RandomForest with threshold: 0.54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "roc_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pr_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "f1_at_thr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "thr",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "051a9ea8-8a60-46a6-8663-669079886935",
              "rows": [
                [
                  "2",
                  "RandomForest",
                  "0.5364215972019819",
                  "0.07025398569475381",
                  "0.0",
                  "0.54"
                ],
                [
                  "5",
                  "LightGBM",
                  "0.5389438665840862",
                  "0.06303570101265939",
                  "0.03856749311294766",
                  "0.32853840420802843"
                ],
                [
                  "1",
                  "DecisionTree",
                  "0.5011396549839697",
                  "0.06156653298523385",
                  "0.06013878180416345",
                  "0.8223919789926045"
                ],
                [
                  "0",
                  "LogisticRegression",
                  "0.5643138844360245",
                  "0.04868884267927722",
                  "0.0",
                  "0.6656480454358362"
                ],
                [
                  "4",
                  "KNN",
                  "0.5240340826289711",
                  "0.04698201123761788",
                  "0.03787878787878788",
                  "0.4"
                ],
                [
                  "3",
                  "NaiveBayes",
                  "0.3776889481929466",
                  "0.03422555492928172",
                  "0.0",
                  "0.9999999999999982"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>f1_at_thr</th>\n",
              "      <th>thr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.536422</td>\n",
              "      <td>0.070254</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.538944</td>\n",
              "      <td>0.063036</td>\n",
              "      <td>0.038567</td>\n",
              "      <td>0.328538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.501140</td>\n",
              "      <td>0.061567</td>\n",
              "      <td>0.060139</td>\n",
              "      <td>0.822392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.564314</td>\n",
              "      <td>0.048689</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.665648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.524034</td>\n",
              "      <td>0.046982</td>\n",
              "      <td>0.037879</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaiveBayes</td>\n",
              "      <td>0.377689</td>\n",
              "      <td>0.034226</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model   roc_auc    pr_auc  f1_at_thr       thr\n",
              "2        RandomForest  0.536422  0.070254   0.000000  0.540000\n",
              "5            LightGBM  0.538944  0.063036   0.038567  0.328538\n",
              "1        DecisionTree  0.501140  0.061567   0.060139  0.822392\n",
              "0  LogisticRegression  0.564314  0.048689   0.000000  0.665648\n",
              "4                 KNN  0.524034  0.046982   0.037879  0.400000\n",
              "3          NaiveBayes  0.377689  0.034226   0.000000  1.000000"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rep_m, best_m_name, best_m_out, all_m_out = compare_custom_models(\n",
        "    Xtr_m, y_train_m.values, Xte_m, y_test_m.values, feature_cols_m, pick_by=\"pr_auc\"\n",
        ")\n",
        "print(\"Best model is\", best_m_name, \"with threshold:\", best_m_out[\"thr\"])\n",
        "rep_m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a187104",
      "metadata": {},
      "source": [
        "## Logs + metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfda8510",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3325144625.py:29: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  train_end = start + pd.Timedelta(min_train_period)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/3325144625.py:31: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  test_end = min(test_start + pd.Timedelta(test_period), end)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aligned split:\n",
            "  TRAIN: 2022-05-02 16:00:00 → 2022-05-03 04:00:00\n",
            "  TEST : 2022-05-03 04:00:00 → 2022-05-03 15:59:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n",
            "/var/folders/9k/3f86r_td4ygg_zflr8hnwkjr0000gn/T/ipykernel_8840/95089035.py:101: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(service_col, group_keys=False).apply(_add)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 310, number of negative: 19179\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19489, number of used features: 137\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015906 -> initscore=-4.124999\n",
            "[LightGBM] [Info] Start training from score -4.124999\n",
            "Best model (logs+metrics): LightGBM | threshold: 0.27674758026313834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/Users/arinagoncharova/Documents/diploma/repo/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "model",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "roc_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "pr_auc",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "f1_at_thr",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "thr",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "417ede76-0f19-4a7a-8452-d5f401e9df9f",
              "rows": [
                [
                  "5",
                  "LightGBM",
                  "0.8433497130301139",
                  "0.06166847275348926",
                  "0.04891304347826087",
                  "0.27674758026313834"
                ],
                [
                  "2",
                  "RandomForest",
                  "0.6925653486864224",
                  "0.05654832254838428",
                  "0.0",
                  "0.55"
                ],
                [
                  "0",
                  "LogisticRegression",
                  "0.845122112290476",
                  "0.04520177546401293",
                  "0.0",
                  "0.835799889807675"
                ],
                [
                  "3",
                  "NaiveBayes",
                  "0.8369194563181404",
                  "0.0450007031359865",
                  "0.08612568967837438",
                  "1.0"
                ],
                [
                  "4",
                  "KNN",
                  "0.5642378037798377",
                  "0.023319483926912993",
                  "0.036",
                  "0.4"
                ],
                [
                  "1",
                  "DecisionTree",
                  "0.5884258230872678",
                  "0.0233157004740358",
                  "0.075",
                  "0.8689666890061456"
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>f1_at_thr</th>\n",
              "      <th>thr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.843350</td>\n",
              "      <td>0.061668</td>\n",
              "      <td>0.048913</td>\n",
              "      <td>0.276748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.692565</td>\n",
              "      <td>0.056548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.845122</td>\n",
              "      <td>0.045202</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.835800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaiveBayes</td>\n",
              "      <td>0.836919</td>\n",
              "      <td>0.045001</td>\n",
              "      <td>0.086126</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.564238</td>\n",
              "      <td>0.023319</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.588426</td>\n",
              "      <td>0.023316</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.868967</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model   roc_auc    pr_auc  f1_at_thr       thr\n",
              "5            LightGBM  0.843350  0.061668   0.048913  0.276748\n",
              "2        RandomForest  0.692565  0.056548   0.000000  0.550000\n",
              "0  LogisticRegression  0.845122  0.045202   0.000000  0.835800\n",
              "3          NaiveBayes  0.836919  0.045001   0.086126  1.000000\n",
              "4                 KNN  0.564238  0.023319   0.036000  0.400000\n",
              "1        DecisionTree  0.588426  0.023316   0.075000  0.868967"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected incidents: 6 / 11 | False alarms: 28\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class WindowConfig:\n",
        "    freq: str = \"1min\"\n",
        "    history_len: int = 5\n",
        "    pre_failure_lead: pd.Timedelta = pd.Timedelta(\"30min\")\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    train_start: pd.Timestamp\n",
        "    train_end: pd.Timestamp\n",
        "    test_start: pd.Timestamp\n",
        "    test_end: pd.Timestamp\n",
        "\n",
        "def build_time_splits_from_range(start: pd.Timestamp, end: pd.Timestamp,\n",
        "                                 min_train_period=\"12H\", test_period=\"12H\") -> List[Split]:\n",
        "    start = pd.to_datetime(start); end = pd.to_datetime(end)\n",
        "    if start >= end:\n",
        "        return []\n",
        "    train_end = start + pd.Timedelta(min_train_period)\n",
        "    test_start = train_end\n",
        "    test_end = min(test_start + pd.Timedelta(test_period), end)\n",
        "    if test_start >= test_end:\n",
        "        return []\n",
        "    return [Split(start, train_end, test_start, test_end)]\n",
        "\n",
        "# normalization and  minimal feature makers\n",
        "def normalize_service_name(s: str) -> Optional[str]:\n",
        "    if s is None or (isinstance(s, float) and pd.isna(s)): return None\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"-(grpc|http|https)$\", \"\", s)   \n",
        "    # s = re.sub(r\"-svc$\", \"\", s) \n",
        "    return s\n",
        "\n",
        "# try/except ensures we reuse them.\n",
        "try:\n",
        "    resample_and_pivot_counts\n",
        "except NameError:\n",
        "    def resample_and_pivot_counts(df, time_col=\"datetime\", cluster_col=\"cluster\", freq=\"1min\", service_col=None):\n",
        "        d = df.copy()\n",
        "        d[time_col] = pd.to_datetime(d[time_col])\n",
        "        keys = [pd.Grouper(key=time_col, freq=freq), cluster_col]\n",
        "        idx = [time_col]\n",
        "        if service_col and service_col in d.columns:\n",
        "            keys = [service_col] + keys\n",
        "            idx = [service_col, time_col]\n",
        "        per_bucket = d.groupby(keys).size().reset_index(name=\"cnt\")\n",
        "        pivot = per_bucket.pivot_table(index=idx, columns=cluster_col, values=\"cnt\",\n",
        "                                       aggfunc=\"sum\", fill_value=0).reset_index()\n",
        "        # rename cluster columns to str\n",
        "        pivot.columns = [str(c) for c in pivot.columns]\n",
        "        return pivot\n",
        "\n",
        "try:\n",
        "    add_history_features\n",
        "except NameError:\n",
        "    def add_history_features(pivot: pd.DataFrame, time_col=\"datetime\", service_col: Optional[str]=None, history_len=5) -> pd.DataFrame:\n",
        "        df = pivot.copy()\n",
        "        df[time_col] = pd.to_datetime(df[time_col])\n",
        "        base = [time_col] + ([service_col] if service_col and service_col in df.columns else [])\n",
        "        feat_cols = [c for c in df.columns if c not in base]\n",
        "        def _add(g: pd.DataFrame):\n",
        "            g = g.sort_values(time_col)\n",
        "            for c in feat_cols:\n",
        "                g[f\"{c}_mean{history_len}\"] = g[c].rolling(history_len, min_periods=1).mean()\n",
        "                g[f\"{c}_diff1\"] = g[c].diff().fillna(0)\n",
        "            return g\n",
        "        if service_col and service_col in df.columns:\n",
        "            df = df.groupby(service_col, group_keys=False).apply(_add)\n",
        "        else:\n",
        "            df = _add(df)\n",
        "        return df\n",
        "\n",
        "try:\n",
        "    make_labels_by_time\n",
        "except NameError:\n",
        "    def make_labels_by_time(times: pd.Series, incidents: pd.DataFrame, lead: pd.Timedelta,\n",
        "                            service: Optional[pd.Series]=None) -> pd.Series:\n",
        "        times = pd.to_datetime(times)\n",
        "        y = pd.Series(0, index=times.index, dtype=int)\n",
        "        inc = incidents.copy()\n",
        "        has_service = \"service\" in inc.columns and inc[\"service\"].notna().any()\n",
        "        if service is not None and has_service:\n",
        "            service = service.astype(str)\n",
        "            for srv, grp in inc.dropna(subset=[\"service\"]).groupby(\"service\"):\n",
        "                mask_srv = (service == str(srv))\n",
        "                if not mask_srv.any(): continue\n",
        "                t_srv = times[mask_srv]\n",
        "                y_srv = pd.Series(0, index=t_srv.index, dtype=int)\n",
        "                for t0 in pd.to_datetime(grp[\"datetime\"]).sort_values():\n",
        "                    win = (t_srv > (t0 - lead)) & (t_srv <= t0)\n",
        "                    if win.any(): y_srv.loc[win] = 1\n",
        "                y.loc[y_srv.index] = y_srv.values\n",
        "        else:\n",
        "            for t0 in pd.to_datetime(inc[\"datetime\"]).sort_values():\n",
        "                win = (times > (t0 - lead)) & (times <= t0)\n",
        "                if win.any():\n",
        "                    y.loc[win.index[win]] = 1\n",
        "        return y\n",
        "\n",
        "# prepare metrics (wide -> per-minute agg by normalized service)\n",
        "#  metrics_df with columns: timestamp|datetime, service or cmdb_id, rr/sr/mrt/count\n",
        "metrics_df = metrics_df.copy()\n",
        "if \"datetime\" not in metrics_df.columns:\n",
        "    metrics_df[\"datetime\"] = pd.to_datetime(metrics_df[\"timestamp\"], unit=\"s\")\n",
        "else:\n",
        "    metrics_df[\"datetime\"] = pd.to_datetime(metrics_df[\"datetime\"])\n",
        "metrics_df = metrics_df.sort_values(\"datetime\")\n",
        "\n",
        "# ensure service, normalize to base\n",
        "if \"service\" not in metrics_df.columns:\n",
        "    if \"cmdb_id\" in metrics_df.columns:\n",
        "        metrics_df = metrics_df.rename(columns={\"cmdb_id\":\"service\"})\n",
        "    else:\n",
        "        metrics_df[\"service\"] = \"GLOBAL\"\n",
        "metrics_df[\"service\"] = metrics_df[\"service\"].map(normalize_service_name)\n",
        "\n",
        "# numeric cast\n",
        "for c in [\"rr\",\"sr\",\"mrt\",\"count\"]:\n",
        "    if c in metrics_df.columns:\n",
        "        metrics_df[c] = pd.to_numeric(metrics_df[c], errors=\"coerce\")\n",
        "\n",
        "# aggregate per minute × service\n",
        "metrics_agg = (\n",
        "    metrics_df\n",
        "    .groupby([pd.Grouper(key=\"datetime\", freq=\"1min\"), \"service\"], as_index=False)\n",
        "    .agg({\"rr\":\"mean\", \"sr\":\"mean\", \"mrt\":\"mean\", \"count\":\"sum\"})\n",
        ")\n",
        "\n",
        "# prepare logs (embeddings -> HDBSCAN clusters → per-minute counts)\n",
        "# df_logs with columns: 'datetime', service_col_name, 'embedding' (array-like)\n",
        "df_logs = df_logs.copy()\n",
        "df_logs[\"datetime\"] = pd.to_datetime(df_logs[\"datetime\"])\n",
        "\n",
        "# normalize service on logs to the same 'service' col\n",
        "if \"service\" not in df_logs.columns:\n",
        "    if 'service' in df_logs.columns:\n",
        "        pass\n",
        "    else:\n",
        "        # use your known service col name variable; if not present, fallback to no service dimension\n",
        "        svc_col = service_col_name if 'service_col_name' in globals() else None\n",
        "        if svc_col and svc_col in df_logs.columns:\n",
        "            df_logs = df_logs.rename(columns={svc_col: \"service\"})\n",
        "        elif \"cmdb_id\" in df_logs.columns:\n",
        "            df_logs = df_logs.rename(columns={\"cmdb_id\":\"service\"})\n",
        "        else:\n",
        "            df_logs[\"service\"] = \"GLOBAL\"\n",
        "df_logs[\"service\"] = df_logs[\"service\"].map(normalize_service_name)\n",
        "\n",
        "# HDBSCAN clustering on train only, then approximate_predict on test\n",
        "import hdbscan\n",
        "from hdbscan import approximate_predict\n",
        "\n",
        "def build_log_features_with_split(df_logs, sp: Split, window_cfg: WindowConfig,\n",
        "                                  cluster_cfg=None, time_col=\"datetime\", embedding_col=\"embedding\", service_col=\"service\"):\n",
        "    # slice\n",
        "    df = df_logs.sort_values(time_col)\n",
        "    train_df = df[(df[time_col] >= sp.train_start) & (df[time_col] < sp.train_end)].copy()\n",
        "    test_df  = df[(df[time_col] >= sp.test_start)  & (df[time_col] < sp.test_end)].copy()\n",
        "\n",
        "    # fit HDBSCAN on train\n",
        "    Xtr = np.vstack(train_df[embedding_col].to_list())\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        metric=getattr(cluster_cfg, \"metric\", \"euclidean\"),\n",
        "        min_cluster_size=getattr(cluster_cfg, \"min_cluster_size\", 800),\n",
        "        min_samples=getattr(cluster_cfg, \"min_samples\", 400),\n",
        "        cluster_selection_method=getattr(cluster_cfg, \"cluster_selection_method\", \"eom\"),\n",
        "        cluster_selection_epsilon=getattr(cluster_cfg, \"cluster_selection_epsilon\", 0.03),\n",
        "        prediction_data=True,\n",
        "        approx_min_span_tree=True,\n",
        "    )\n",
        "    fit_max = getattr(cluster_cfg, \"fit_max_points\", None)\n",
        "    if fit_max and Xtr.shape[0] > fit_max:\n",
        "        idx = np.random.RandomState(0).choice(Xtr.shape[0], fit_max, replace=False)\n",
        "        clusterer.fit(Xtr[idx])\n",
        "    else:\n",
        "        clusterer.fit(Xtr)\n",
        "\n",
        "    tr_labels, _ = approximate_predict(clusterer, Xtr)\n",
        "    train_df[\"cluster\"] = tr_labels\n",
        "\n",
        "    Xte = np.vstack(test_df[embedding_col].to_list())\n",
        "    te_labels, _ = approximate_predict(clusterer, Xte)\n",
        "    test_df[\"cluster\"] = te_labels\n",
        "\n",
        "    # resample -> pivot counts per minute×service\n",
        "    pivot_train = resample_and_pivot_counts(train_df, time_col=time_col, cluster_col=\"cluster\",\n",
        "                                            freq=window_cfg.freq, service_col=service_col)\n",
        "    pivot_test  = resample_and_pivot_counts(test_df,  time_col=time_col, cluster_col=\"cluster\",\n",
        "                                            freq=window_cfg.freq, service_col=service_col)\n",
        "\n",
        "    # add history features\n",
        "    feat_train = add_history_features(pivot_train, time_col=time_col, service_col=service_col,\n",
        "                                      history_len=window_cfg.history_len)\n",
        "    feat_test  = add_history_features(pivot_test,  time_col=time_col, service_col=service_col,\n",
        "                                      history_len=window_cfg.history_len)\n",
        "    return feat_train, feat_test\n",
        "\n",
        "# build 1 aligned time split (intersection of logs & metrics ranges)\n",
        "log_min, log_max = df_logs[\"datetime\"].min(), df_logs[\"datetime\"].max()\n",
        "met_min, met_max = metrics_agg[\"datetime\"].min(), metrics_agg[\"datetime\"].max()\n",
        "common_start = max(log_min, met_min)\n",
        "common_end   = min(log_max, met_max)\n",
        "\n",
        "min_train_period = \"12H\"\n",
        "test_period = \"12H\"\n",
        "splits = build_time_splits_from_range(common_start, common_end, min_train_period, test_period)\n",
        "if not splits:\n",
        "    raise ValueError(\"Not enough overlapping data between logs and metrics for the requested windows.\")\n",
        "sp = splits[0]\n",
        "print(\"Aligned split:\")\n",
        "print(\"  TRAIN:\", sp.train_start, \"→\", sp.train_end)\n",
        "print(\"  TEST :\", sp.test_start,  \"→\", sp.test_end)\n",
        "\n",
        "# calculate features\n",
        "wcfg = WindowConfig(freq=\"1min\", history_len=5, pre_failure_lead=pd.Timedelta(\"30min\"))\n",
        "\n",
        "# logs -> features in aligned windows\n",
        "feat_train_l, feat_test_l = build_log_features_with_split(df_logs, sp, wcfg, cluster_cfg=None,\n",
        "                                                          time_col=\"datetime\", embedding_col=\"embedding\", service_col=\"service\")\n",
        "\n",
        "# metrics -> just slice per window and add history\n",
        "def slice_by_split(df, sp, time_col=\"datetime\"):\n",
        "    return (df[(df[time_col] >= sp.train_start) & (df[time_col] < sp.train_end)].copy(),\n",
        "            df[(df[time_col] >= sp.test_start)  & (df[time_col] < sp.test_end)].copy())\n",
        "\n",
        "met_train, met_test = slice_by_split(metrics_agg, sp, time_col=\"datetime\")\n",
        "feat_train_m = add_history_features(met_train, time_col=\"datetime\", service_col=\"service\", history_len=wcfg.history_len)\n",
        "feat_test_m  = add_history_features(met_test,  time_col=\"datetime\", service_col=\"service\", history_len=wcfg.history_len)\n",
        "\n",
        "# merge LOG + metrics features on [\"datetime\",\"service\"] and relabel\n",
        "base_keys = [\"datetime\",\"service\"]\n",
        "train_merged = pd.merge(feat_train_l, feat_train_m, on=base_keys, how=\"outer\").sort_values(\"datetime\").fillna(0)\n",
        "test_merged  = pd.merge(feat_test_l,  feat_test_m,  on=base_keys, how=\"outer\").sort_values(\"datetime\").fillna(0)\n",
        "\n",
        "# normalize incidents & services\n",
        "inc = service_failures_03_05_df.copy()\n",
        "inc[\"datetime\"] = pd.to_datetime(inc[\"datetime\"])\n",
        "if \"service\" in inc.columns:\n",
        "    inc[\"service\"] = inc[\"service\"].map(normalize_service_name)\n",
        "elif \"cmdb_id\" in inc.columns:\n",
        "    inc[\"service\"] = inc[\"cmdb_id\"].map(normalize_service_name)\n",
        "else:\n",
        "    inc[\"service\"] = None  # fall back to global if no service info\n",
        "\n",
        "# labels on merged grid\n",
        "y_train_c = make_labels_by_time(\n",
        "    train_merged[\"datetime\"], inc, wcfg.pre_failure_lead,\n",
        "    service=train_merged[\"service\"] if \"service\" in train_merged.columns else None\n",
        ")\n",
        "y_test_c = make_labels_by_time(\n",
        "    test_merged[\"datetime\"], inc, wcfg.pre_failure_lead,\n",
        "    service=test_merged[\"service\"] if \"service\" in test_merged.columns else None\n",
        ")\n",
        "\n",
        "# train/evaluate sklearn models on merged features\n",
        "# build X matrices\n",
        "drop_cols = base_keys\n",
        "feature_cols_c = [c for c in train_merged.columns if c not in drop_cols]\n",
        "Xtr_c = train_merged[feature_cols_c].values\n",
        "Xte_c = test_merged[feature_cols_c].values\n",
        "\n",
        "# compare_custom_models must already be defined (we added it earlier in your notebook)\n",
        "rep_c, best_c_name, best_c_out, all_c_out = compare_custom_models(\n",
        "    Xtr_c, y_train_c.values, Xte_c, y_test_c.values, feature_cols_c, pick_by=\"pr_auc\"\n",
        ")\n",
        "print(\"Best model (logs+metrics):\", best_c_name, \"| threshold:\", best_c_out[\"thr\"])\n",
        "display(rep_c)\n",
        "\n",
        "# prediction results: detected/missed/false alarms\n",
        "timeline = test_merged[[\"datetime\"]].copy()\n",
        "timeline[\"y_true\"] = y_test_c.values.astype(int)\n",
        "timeline[\"proba\"]  = best_c_out[\"proba_test\"]\n",
        "timeline[\"pred\"]   = best_c_out[\"pred_test\"]\n",
        "\n",
        "lead = wcfg.pre_failure_lead\n",
        "inc_test = inc[(inc[\"datetime\"] >= timeline[\"datetime\"].min()) & (inc[\"datetime\"] <= timeline[\"datetime\"].max())].copy()\n",
        "inc_test[\"first_fire\"] = pd.NaT\n",
        "inc_test[\"lead_minutes\"] = np.nan\n",
        "for i, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    mask = (timeline[\"datetime\"] > t0 - lead) & (timeline[\"datetime\"] <= t0) & (timeline[\"pred\"] == 1)\n",
        "    if mask.any():\n",
        "        t_first = timeline.loc[mask, \"datetime\"].min()\n",
        "        inc_test.at[i, \"first_fire\"] = t_first\n",
        "        inc_test.at[i, \"lead_minutes\"] = (t0 - t_first).total_seconds()/60.0\n",
        "\n",
        "detected = inc_test[inc_test[\"first_fire\"].notna()].copy()\n",
        "missed   = inc_test[inc_test[\"first_fire\"].isna()].copy()\n",
        "fires = timeline[timeline[\"pred\"] == 1].copy()\n",
        "true_alert_mask = pd.Series(False, index=fires.index)\n",
        "for _, row in inc_test.iterrows():\n",
        "    t0 = pd.to_datetime(row[\"datetime\"])\n",
        "    true_alert_mask |= ((fires[\"datetime\"] > t0 - lead) & (fires[\"datetime\"] <= t0))\n",
        "false_alarms = fires[~true_alert_mask].copy()\n",
        "\n",
        "print(f\"Detected incidents: {len(detected)} / {len(inc_test)} | False alarms: {len(false_alarms)}\")\n",
        "\n",
        "# save artifacts\n",
        "timeline.to_csv(\"combo_pred_timeline.csv\", index=False)\n",
        "detected[[\"datetime\",\"first_fire\",\"lead_minutes\"]].to_csv(\"combo_incidents_detected.csv\", index=False)\n",
        "missed[[\"datetime\"]].to_csv(\"combo_incidents_missed.csv\", index=False)\n",
        "false_alarms[[\"datetime\"]].to_csv(\"combo_false_alarms.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
